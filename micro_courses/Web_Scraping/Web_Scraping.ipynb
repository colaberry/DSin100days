{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Web Scraping\n",
    "\n",
    "## 1. What is Web Scraping\n",
    "\n",
    "In any given field, there is a growing amount of data collected and presented all over the web. Some of your projects might need access to the data presented at various websites. For example, you might want to collect the stock prices information presented at Bloomberg or Google Finance website. You could casually browse these websites to get the information you are looking for. How  do you allow your program to structurally read these websites and consume the information the applications need? Web Scraping helps you in retrieving such information the program needs in a meaningful way.\n",
    "\n",
    "In this micro course we will learn how to use Python to do Web Scraping.\n",
    "\n",
    "If we want to create the Web Scraping programs mentioned earlier, we first need to understand the structure of a typical web page/website. We will explain the details later. \n",
    "\n",
    "In addition to understanding what you can expect in a web page, you need tools to extract information from such web pages. These could be libraries exposed to you in any programming language. In python, there are several libraries, one of which is **Beautiful Soup**. In the subsequent sections we will describe how to use such a library in your program.   \n",
    "\n",
    "\n",
    "### a) Structure of a Web Page\n",
    "\n",
    "A Website is really what the designer of the website thinks, an abstraction. The website is a collection of individual pages. It can change from one to another, and the type can be an aplication or a static page. However, over the years as the web technology matured, there are some generic layers to a webpage that you come to expect. A webpage typically contains:\n",
    "```\n",
    "Header\n",
    "Navigation\n",
    "Bread Crumb\n",
    "Tab Navigation\n",
    "Content Pane\n",
    "    Left Navigation Links\n",
    "    Main content\n",
    "    Right Details\n",
    "Footer\n",
    "```\n",
    "In most cases, we are interested in the content pane to extract the most useful information from the websites.\n",
    "\n",
    "\n",
    "### b) Http Request\n",
    "\n",
    "Web pages are accessed using http requests. Python provides a module called **requests**.\n",
    "The requests module allows a python program to read a web page by taking in a URL string.\n",
    "\n",
    "``` python\n",
    "import requests\n",
    "response = requests.get(\"http://www.google.com\")\n",
    "html = response.content\n",
    "```\n",
    "\n",
    "As seen above, `import` imports the requests library. The `get` method takes a URL string and gets the html page from the website referred by the URL. The `content` shows the raw bytes of the response's content.\n",
    "\n",
    "In the following exercise, we will create a code snippet that reads a page and prints out the html content.\n",
    "In this example, we will use a web site called http://quotes.toscrape.com/ which is specifically designed as a play ground for web scraping. This site has a list of nice quotes by famous authors. The quotes are presented in the web page as tables and div's. The idea is to read the quotes and the corresponding authors from the page. In addition, we could extract additional information about the author, such as the authors' date of birth. This information is provided in a separate author's page.\n",
    "\n",
    "After we opened the above website, we can right click and select \"Inspect\" to reveal the html content. We can find many \"tags\" on this page, ```<head>, <body>, <div>, <span>```, etc. These tags define the structure of the web page. We will only focus on ```<body>``` while scraping because all the information is usually under this tag. Here is a complete list of [HTML tags](https://www.w3schools.com/TAGS/default.ASP). For most tags, after using there will be a closing tag with **/**, like ```<div>...</div>``` or ```<span>...</span>```. Between the opening and closing tags, it is the actual content which we will be focusing to scrape.\n",
    "\n",
    "<img src=Quotes_to_Scrape.jpg width=\"900\"/>\n",
    "\n",
    "As you've noticed, in some tags there are other elements such as ```class, itemprop```, etc. These are the attributes of the tag, which define the tag's behaviors, styles, etc. \n",
    "\n",
    "In the following sections, we will read all of these information for each quote and create our own quotes dataset.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write code to open url http://quotes.toscrape.com/ and print the html content. You can practice in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Parsing Website Content using Beautiful Soup\n",
    "\n",
    "\n",
    "There are several libraries available in Python for the purpose of web scraping such as Beautifu Soup, Scrapy etc. In this section we will learn how to use **Beautiful Soup** to scrape a web site.\n",
    "\n",
    "### a) What is Beautiful Soup\n",
    "\n",
    "**Beautiful Soup** is a Python library for pulling data out of HTML and XML files. The main class is BeautifulSoup which is part of a library bs4 (BeautifulSoup 4.0). In order to start using BeautifulSoup, we need to import BeautifulSoup from bs4.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "If there's an error preventing you from `import` the library, it's probably that you didn't have the **bs4** library installed yet. You can simply run the following code before the `import` code:\n",
    "```python\n",
    "!pip install bs4\n",
    "```\n",
    "\n",
    "### b) Parsing the web pages \n",
    "\n",
    "In order to start using the module for parsing web pages, you start by passing the html document to BeautifulSoup.\n",
    "For example:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<a href=\"http://www.google.com\">Click to visit google.com</a>')\n",
    "```\n",
    "\n",
    "\n",
    "### c) Reading the content using tags and classes\n",
    "\n",
    "We can read a tag by using that tag's name. For example, `soup.a` will return the first `<a></a>` tag.\n",
    "\n",
    "```python\n",
    "a_tag = soup.a\n",
    "print(a_tag)\n",
    "```\n",
    "If we want to find all the `<div></div>` tags, along with specific class `my_class`, we can use `find_all()` with `class_` attribute as below.\n",
    "```python\n",
    "specific_tags = soup.find_all(\"div\", class_=\"my_class\")\n",
    "\n",
    "```\n",
    "Note here, `class` is a reserved word in Python so we need to add `_` after the key word to use it properly. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write code to open http://quotes.toscrape.com using BeautifulSoup (Hint: Refer to the first section). Read and print all html tags (div) with class as \"quote\". Save it in variable 'tag'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting to the detailed tags in HTML\n",
    "\n",
    "Once we find a way to parse the html, the next step is to get to the individual tags that we are interested in.\n",
    "\n",
    "### a) Class and Tags\n",
    "\n",
    "Beautiful Soup provides functions to obtain specific tags and search for tags and class.\n",
    "\n",
    "```python\n",
    "tag = html_text.find(\"div\", class_=\"myclass\")\n",
    "```\n",
    "\n",
    "In the above example, we can find the specific tag , `<div></div>` in this case, where the class is `myclass`. This fetches the first such result. If we use `find_all()` function that will provide all such tags as a result of the search.\n",
    "\n",
    "### b) Getting the content within\n",
    "\n",
    "In order to fetch the content, use tag`.text`.<br>\n",
    "For example, we can read the content of this html tag `<a href=\"http://www.google.com\">Click to visit google.com</a>` by using `.text`, which will return `Click to visit google.com`. in this case the code will be similar to below:\n",
    "\n",
    "```python\n",
    "    content = tag.text\n",
    "    print(\"Content is :: \", content)\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the previous exercise 'tag' variable with all `<div>` with class `quote`, find all \"small\" tags with the class as  \"author\". Print the content within the resulting div tags. <br>\n",
    "(Hint: First we need to iterate over the variable 'tag', because it is a list and we need to check each list item using **for loop**. Then we need to extract the text for `small` tags with class `author`, and the text for `span` tags.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "```python\n",
    "for t in tag:\n",
    "    print(t.span.text)\n",
    "    a = t.find(\"small\", class_=\"author\")\n",
    "    author = a.text\n",
    "    print(\"By :: \", author)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading Sub-Links and Joining data\n",
    "\n",
    "In real world, you might need to gather information from multiple sources or at the very least multiple pages or urls from the same application to get the related information. Here is an example of how to do it using **BeautifulSoup**.\n",
    "\n",
    "We obtain the hrefs from anchor tags (`<a></a>`) in two steps. The first step is to obtain the anchors by calling `.a` on the base html doc. Then on the anchor element obtain the href (link) by calling `.get('href')`.\n",
    "\n",
    "```python\n",
    "    hrefs = soup.a\n",
    "    link = hrefs.get('href')\n",
    "    \n",
    "```\n",
    "\n",
    "Then you could call the requests.get(link) to obtain the sub-link page and perform fresh parsing of the related content.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the quotes example, write a function get_author_link() and for each parent author class, pass the link and obtain the author brith-born-date and print them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T05:18:34.670326Z",
     "start_time": "2020-04-29T05:18:34.512433Z"
    }
   },
   "outputs": [],
   "source": [
    "#Modify the code below\n",
    "def get_author_dob(link_url):\n",
    "    response_auth = requests.get(link_url)\n",
    "##add your function code here\n",
    "    \n",
    "    \n",
    "soup = BeautifulSoup(html)\n",
    "tag = soup.find_all(\"div\", class_=\"quote\")\n",
    "#print(tag)\n",
    "for t in tag:\n",
    "    text_out =  t.span.text \n",
    "## Add your logic to call the function get_author_dob() with links\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "\n",
    "def get_author_dob(link_url):\n",
    "    response_auth = requests.get(link_url)\n",
    "    html_auth = response_auth.content\n",
    "    auth_soup = BeautifulSoup(html_auth)\n",
    "    auth_tag = auth_soup.find(\"span\", class_=\"author-born-date\")\n",
    "    return auth_tag.text\n",
    "\n",
    "output = []\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "tag = soup.find_all(\"div\", class_=\"quote\")\n",
    "#print(tag)\n",
    "for t in tag:\n",
    "    text_out =  t.span.text \n",
    "    print(t.span.text)\n",
    "    \n",
    "    a = t.find(\"small\", class_=\"author\")\n",
    "    author = a.text\n",
    "    text_out = text_out + ',\"' + author + '\"' \n",
    "    print(\"By :: \", author)\n",
    "    hrefs = t.a\n",
    "    link = hrefs.get('href')\n",
    "    link_url = url+ link\n",
    "    print(link_url)\n",
    "    dob = get_author_dob(link_url)\n",
    "    print(\"Author DOB:\", dob)\n",
    "    text_out = text_out + ',\"' + dob + '\"' \n",
    "    output.append(text_out)\n",
    "    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Writing to file\n",
    "\n",
    "When we are done with the scraping operations we can write the data to a file. <br>\n",
    "(If you are not familiar with this operation, feel free to skip this section. We will talk about `file_IO` in future micro courses.)\n",
    "\n",
    "```python\n",
    "file_wr = open(\"output.csv\", \"a\")\n",
    "for line in output:\n",
    "    file_wr.write(str(line))\n",
    "file_wr.close()\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the previous exercise with \"output\" from quotes, write all lines to a \"quotes.csv\" file by appending each line from scraped quotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn more about Web Scraping using Python\n",
    "Now you know the basics of scraping data from websites. It gives you more freedom to get data compared with using APIs. However, it may not be stable because a number of websites may block the access to your program from time to time. If you want to learn more about how to deal with this problem and how to use third party APIs to retrieve data, check out our course at https://refactored.ai. Our course on python covers everything from introductory python to pandas, to data visualization with Plotly, to statistics and machine learning techniques."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
