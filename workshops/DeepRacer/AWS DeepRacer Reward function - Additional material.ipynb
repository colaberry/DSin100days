{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Racing tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will elaborate more on the ways we can tweak our reward function to generate faster lap times as well as other methods associated with the DeepRacer which can help in developing a faster racecar\n",
    "\n",
    "__Before you begin with this material we would recommend you to go over the vehicle tuning notebook as it will lay some basic foundation to this document__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Contents:__\n",
    "1. Reward function<br>\n",
    "    1.1 Reward function Inputs<br>\n",
    "    1.2 AWS DeepRacer Reward Function Examples - Basic<br>\n",
    "    1.3 AWS DeepRacer Reward Function Examples - Advanced\n",
    "2. Additonal information and tips\n",
    "3. Hyperparameters + tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reward function describes immediate feedback (as a score for reward or penalty) when the vehicle takes an action to move from a given position on the track to a new position. Its purpose is to encourage the vehicle to make moves along the track to reach its destination quickly. The model training process will attempt to find a policy which maximizes the average total reward the vehicle experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Reward function Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AWS DeepRacer reward function takes a dictionary object as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params) :\n",
    "    \n",
    "    reward = ...\n",
    "\n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The params dictionary object contains the following key-value pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"all_wheels_on_track\": Boolean,        # flag to indicate if the agent is on the track\n",
    "    \"x\": float,                            # agent's x-coordinate in meters\n",
    "    \"y\": float,                            # agent's y-coordinate in meters\n",
    "    \"closest_objects\": [int, int],         # zero-based indices of the two closest objects to the agent's current position of (x, y).\n",
    "    \"closest_waypoints\": [int, int],       # indices of the two nearest waypoints.\n",
    "    \"distance_from_center\": float,         # distance in meters from the track center \n",
    "    \"is_crashed\": Boolean,                 # Boolean flag to indicate whether the agent has crashed.\n",
    "    \"is_left_of_center\": Boolean,          # Flag to indicate if the agent is on the left side to the track center or not. \n",
    "    \"is_offtrack\": Boolean,                # Boolean flag to indicate whether the agent has gone off track.\n",
    "    \"is_reversed\": Boolean,                # flag to indicate if the agent is driving clockwise (True) or counter clockwise (False).\n",
    "    \"heading\": float,                      # agent's yaw in degrees\n",
    "    \"objects_distance\": [float, ],         # list of the objects' distances in meters between 0 and track_length in relation to the starting line.\n",
    "    \"objects_heading\": [float, ],          # list of the objects' headings in degrees between -180 and 180.\n",
    "    \"objects_left_of_center\": [Boolean, ], # list of Boolean flags indicating whether elements' objects are left of the center (True) or not (False).\n",
    "    \"objects_location\": [(float, float),], # list of object locations [(x,y), ...].\n",
    "    \"objects_speed\": [float, ],            # list of the objects' speeds in meters per second.\n",
    "    \"progress\": float,                     # percentage of track completed\n",
    "    \"speed\": float,                        # agent's speed in meters per second (m/s)\n",
    "    \"steering_angle\": float,               # agent's steering angle in degrees\n",
    "    \"steps\": int,                          # number steps completed\n",
    "    \"track_length\": float,                 # track length in meters.\n",
    "    \"track_width\": float,                  # width of the track\n",
    "    \"waypoints\": [(float, float), ]        # list of (x,y) as milestones along the track center\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed technical reference of the input parameters is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__all_wheels_on_track__\n",
    "\n",
    "__Type__: Boolean\n",
    "\n",
    "__Range__: (True:False)\n",
    "\n",
    "A Boolean flag to indicate whether the agent is on-track or off-track. It's off-track (False) if any of its wheels are outside of the track borders. It's on-track (True) if all of the wheels are inside the two track borders. The following illustration shows that the agent is on-track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-all_wheels_on_track-true.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following illustration shows that the agent is off-track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-all_wheels_on_track-false.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the all_wheels_on_track parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "define reward_function(params):\n",
    "    #############################################################################\n",
    "    '''\n",
    "    Example of using all_wheels_on_track and speed\n",
    "    '''\n",
    "\n",
    "    # Read input variables\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    speed = params['speed']\n",
    "\n",
    "    # Set the speed threshold based your action space \n",
    "    SPEED_THRESHOLD = 1.0 \n",
    "\n",
    "    if not all_wheels_on_track:\n",
    "        # Penalize if the car goes off track\n",
    "        reward = 1e-3\n",
    "    elif speed < SPEED_THRESHOLD:\n",
    "        # Penalize if the car goes too slow\n",
    "        reward = 0.5\n",
    "    else:\n",
    "        # High reward if the car stays on track and goes fast\n",
    "        reward = 1.0\n",
    "\n",
    "    return reward`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramaters:\n",
    "    \n",
    "__waypoints__\n",
    "\n",
    "__Type__: list of [float, float]\n",
    "\n",
    "__Range__: [[$x_{w,0}$,$y_{w,0}$] … [$x_{w,Max-1}$, $y_{w,Max-1}$]]\n",
    "\n",
    "An ordered list of track-dependent `Max` milestones along the track center. Each milestone is described by a coordinate of ($x_{w,i}$, $y_{w,i}$). For a looped track, the first and last waypoints are the same. For a straight or other non-looped track, the first and last waypoints are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-waypoints.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the waypoints parameter was used in the __closest_waypoints__ example below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__closest_waypoints__\n",
    "\n",
    "__Type__: [int, int]\n",
    "\n",
    "__Range__: [(0:Max-1),(1:Max-1)]\n",
    "\n",
    "The zero-based indices of the two neighboring waypoints closest to the agent's current position of (x, y). The distance is measured by the Euclidean distance from the center of the agent. The first element refers to the closest waypoint behind the agent and the second element refers the closest waypoint in front of the agent. Max is the length of the waypoints list. In the illustration shown in waypoints, the closest_waypoints would be [16, 17].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-waypoints.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__: A reward function using the closest_waypoints parameter.\n",
    "\n",
    "The following example reward function demonstrates how to use waypoints and closest_waypoints as well as heading to calculate immediate rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    ###############################################################################\n",
    "    '''\n",
    "    Example of using waypoints and heading to make the car in the right direction\n",
    "    '''\n",
    "\n",
    "    import math\n",
    "\n",
    "    # Read input variables\n",
    "    waypoints = params['waypoints']\n",
    "    closest_waypoints = params['closest_waypoints']\n",
    "    heading = params['heading']\n",
    "\n",
    "    # Initialize the reward with typical value \n",
    "    reward = 1.0\n",
    "\n",
    "    # Calculate the direction of the center line based on the closest waypoints\n",
    "    next_point = waypoints[closest_waypoints[1]]\n",
    "    prev_point = waypoints[closest_waypoints[0]]\n",
    "\n",
    "    # Calculate the direction in radius, arctan2(dy, dx), the result is (-pi, pi) in radians\n",
    "    track_direction = math.atan2(next_point[1] - prev_point[1], next_point[0] - prev_point[0]) \n",
    "    # Convert to degree\n",
    "    track_direction = math.degrees(track_direction)\n",
    "\n",
    "    # Calculate the difference between the track direction and the heading direction of the car\n",
    "    direction_diff = abs(track_direction - heading)\n",
    "    if direction_diff > 180:\n",
    "        direction_diff = 360 - direction_diff\n",
    "\n",
    "    # Penalize the reward if the difference is too large\n",
    "    DIRECTION_THRESHOLD = 10.0\n",
    "    if direction_diff > DIRECTION_THRESHOLD:\n",
    "        reward *= 0.5\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "    \n",
    "__heading__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: -180:+180\n",
    "\n",
    "Heading direction, in degrees, of the agent with respect to the x-axis of the coordinate system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-heading.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the heading parameter can be seen in the __closest_waypoints__ example above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__closest_objects__\n",
    "\n",
    "__Type__: [int, int]\n",
    "\n",
    "__Range__: [(0:len(object_locations)-1), (0:len(object_locations)-1]\n",
    "\n",
    "The zero-based indices of the two closest objects to the agent's current position of (x, y). The first index refers to the closest object behind the agent, and the second index refers to the closest object in front of the agent. If there is only one object, both indices are 0.\n",
    "\n",
    "__Note__: This is primarily used for the object detection race in the AWS DeepRacer. For time trial race this can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__distance_from_center__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: 0:~track_width/2\n",
    "\n",
    "Displacement, in meters, between the agent center and the track center. The observable maximum displacement occurs when any of the agent's wheels are outside a track border and, depending on the width of the track border, can be slightly smaller or larger than half the track_width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-distance_from_center.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the distance_from_center parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    #################################################################################\n",
    "    '''\n",
    "    Example of using distance from the center \n",
    "    ''' \n",
    "\n",
    "    # Read input variable\n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "\n",
    "    # Penalize if the car is too far away from the center\n",
    "    marker_1 = 0.1 * track_width\n",
    "    marker_2 = 0.5 * track_width\n",
    "\n",
    "    if distance_from_center <= marker_1:\n",
    "        reward = 1.0\n",
    "    elif distance_from_center <= marker_2:\n",
    "        reward = 0.5\n",
    "    else:\n",
    "        reward = 1e-3  # likely crashed/ close to off track\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "\n",
    "__is_crashed__\n",
    "\n",
    "__Type__: Boolean\n",
    "\n",
    "__Range__: (True:False)\n",
    "\n",
    "A Boolean flag to indicate whether the agent has crashed into another object (True) or not (False) as a termination status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "\n",
    "__is_left_of_center__\n",
    "\n",
    "__Type__: Boolean\n",
    "\n",
    "__Range__: (True : False)\n",
    "\n",
    "A Boolean flag to indicate if the agent is on the left side to the track center (True) or on the right side (False)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "\n",
    "__is_offtrack__\n",
    "\n",
    "__Type__: Boolean\n",
    "\n",
    "__Range__: (True:False)\n",
    "\n",
    "A Boolean flag to indicate whether the agent has off track (True) or not (False) as a termination status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "\n",
    "__is_reversed__\n",
    "\n",
    "__Type__: Boolean\n",
    "\n",
    "__Range__: (True:False)\n",
    "\n",
    "A Boolean flag to indicate if the agent is driving on clock-wise (True) or counter clock-wise (False).\n",
    "\n",
    "It's used when you enable direction change for each episode. An episode is a period in which the vehicle starts from a given starting point and ends up completing the track or going off the track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "\n",
    "__objects_distance__\n",
    "\n",
    "__Type__: [float, … ]\n",
    "\n",
    "__Range__: [(0:track_length), … ]\n",
    "\n",
    "A list of the distances between objects in the environment in relation to the starting line. The $i_{th}$ element measures the distance in meters between the $i_{th}$ object and the starting line along the track center line.\n",
    "\n",
    "To index the distance between a single object and the agent, use:\n",
    "\n",
    "`abs(params[\"objects_distance\"][index] - (params[\"progress\"]/100.0)*params[\"track_length\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/objects-distance-diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Note__\n",
    "\n",
    "abs | (var1) - (var2)| = how close the car is to an object, WHEN var1 = [\"objects_distance\"][index] and var2 = params[\"progress\"]*params[\"track_length\"]\n",
    "\n",
    "To get an index of the closest object in front of the vehicle and the closest object behind the vehicle, use the \"closest_objects\" parameter.\n",
    "\n",
    "\n",
    "* This is primarily used for the object detection race in the AWS DeepRacer. For time trial race this can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__objects_heading__\n",
    "\n",
    "__Type__: [float, … ]\n",
    "\n",
    "__Range__: [(-180:180), … ]\n",
    "\n",
    "List of the headings of objects in degrees. The $i_{th}$ element measures the heading of the $i_{th}$ object. For stationary objects, their headings are 0. For a bot vehicle, the corresponding element's value is the vehicle's heading angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__objects_left_of_center__\n",
    "\n",
    "__Type__: [Boolean, … ]\n",
    "\n",
    "__Range__: [True|False, … ]\n",
    "\n",
    "List of Boolean flags. The $i_{th}$ element value indicates whether the $i_{th}$ object is to the left (True) or right (False) side of the track center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__objects_location__\n",
    "\n",
    "__Type__: [(x,y), ...]\n",
    "\n",
    "__Range__: [(0:N,0:N), ...]\n",
    "\n",
    "List of all object locations, each location is a tuple of (x, y).\n",
    "\n",
    "The size of the list equals the number of objects on the track. Note the object could be the stationary obstacles, moving bot vehicles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__objects_speed__\n",
    "\n",
    "__Type__: [float, … ]\n",
    "\n",
    "__Range__: [(0:12.0), … ]\n",
    "\n",
    "List of speeds (meters per second) for the objects on the track. For stationary objects, their speeds are 0. For a bot vehicle, the value is the speed you set in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__progress__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: 0:100\n",
    "\n",
    "Percentage of track completed.\n",
    "\n",
    "Example: A reward function using the progress parameter is shared below in the _steps_ example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "    \n",
    "__speed__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: 0.0:5.0\n",
    "\n",
    "The observed speed of the agent, in meters per second (m/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-speed.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the speed parameter was the initial __all_wheels_on_track__ example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "\n",
    "__steering_angle__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: -30:30\n",
    "\n",
    "Steering angle, in degrees, of the front wheels from the center line of the agent. The negative sign (-) means steering to the right and the positive (+) sign means steering to the left. The agent center line is not necessarily parallel with the track center line as is shown in the following illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-steering.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the steering_angle parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of using steering angle\n",
    "    '''\n",
    "\n",
    "    # Read input variable\n",
    "    steering = abs(params['steering_angle']) # We don't care whether it is left or right steering\n",
    "\n",
    "    # Initialize the reward with typical value \n",
    "    reward = 1.0\n",
    "\n",
    "    # Penalize if car steer too much to prevent zigzag\n",
    "    STEERING_THRESHOLD = 20.0\n",
    "    if steering > ABS_STEERING_THRESHOLD:\n",
    "        reward *= 0.8\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__steps__\n",
    "\n",
    "__Type__: int\n",
    "\n",
    "__Range__: 0:Nstep\n",
    "\n",
    "Number of steps completed. A step corresponds to an action taken by the agent following the current policy.\n",
    "\n",
    "Example: A reward function using the steps parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    #############################################################################\n",
    "    '''\n",
    "    Example of using steps and progress\n",
    "    '''\n",
    "\n",
    "    # Read input variable\n",
    "    steps = params['steps']\n",
    "    progress = params['progress']\n",
    "    \n",
    "    # Total num of steps we want the car to finish the lap, it will vary depends on the track length\n",
    "    TOTAL_NUM_STEPS = 300\n",
    "\n",
    "    # Initialize the reward with typical value \n",
    "    reward = 1.0\n",
    "\n",
    "    # Give additional reward if the car pass every 100 steps faster than expected \n",
    "    if (steps % 100) == 0 and progress > (steps / TOTAL_NUM_STEPS) * 100 :\n",
    "        reward += 10.0\n",
    "\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Paramater:\n",
    "    \n",
    "__track_length__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: [0:Lmax]\n",
    "\n",
    "The track length in meters. Lmax is track-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameter:\n",
    "\n",
    "__track_width__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: 0:Dtrack\n",
    "\n",
    "Track width in meters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-track_width.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: A reward function using the track_width parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    #############################################################################\n",
    "    '''\n",
    "    Example of using track width\n",
    "    '''\n",
    "\n",
    "    # Read input variable\n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "\n",
    "    # Calculate the distance from each border\n",
    "    distance_from_border = 0.5 * track_width - distance_from_center\n",
    "\n",
    "    # Reward higher if the car stays inside the track borders\n",
    "    if distance_from_border >= 0.05:\n",
    "        reward *= 1.0\n",
    "    else:\n",
    "        reward = 1e-3 # Low reward if too close to the border or goes off the track\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Parameters:\n",
    "\n",
    "__x, y__\n",
    "\n",
    "__Type__: float\n",
    "\n",
    "__Range__: 0:N\n",
    "\n",
    "Location, in meters, of the agent center along the x and y axes, of the simulated environment containing the track. The origin is at the lower-left corner of the simulated environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepracer-reward-function-input-x-y.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 AWS DeepRacer Reward Function Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists some examples of the AWS DeepRacer reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Topics__\n",
    "\n",
    "Example 1: Follow the Center Line in Time Trials\n",
    "\n",
    "Example 2: Stay Inside the Two Borders in Time Trials\n",
    "\n",
    "Example 3: Prevent Zig-Zag in Time Trials\n",
    "\n",
    "Example 4: Stay On One Lane without Crashing into Stationary Obstacles or Moving Vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 1: Follow the Center Line in Time Trials__\n",
    "\n",
    "This example determines how far away the agent is from the center line, and gives higher reward if it is closer to the center of the track, encouraging the agent to closely follow the center line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to follow center line\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "\n",
    "    # Calculate 3 markers that are increasingly further away from the center line\n",
    "    marker_1 = 0.1 * track_width\n",
    "    marker_2 = 0.25 * track_width\n",
    "    marker_3 = 0.5 * track_width\n",
    "\n",
    "    # Give higher reward if the car is closer to center line and vice versa\n",
    "    if distance_from_center <= marker_1:\n",
    "        reward = 1\n",
    "    elif distance_from_center <= marker_2:\n",
    "        reward = 0.5\n",
    "    elif distance_from_center <= marker_3:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        reward = 1e-3  # likely crashed/ close to off track\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 2: Stay Inside the Two Borders in Time Trials__\n",
    "\n",
    "This example simply gives high rewards if the agent stays inside the borders, and let the agent figure out what is the best path to finish a lap. It is easy to program and understand, but likely takes longer to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to stay inside the two borders of the track\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    \n",
    "    # Give a very low reward by default\n",
    "    reward = 1e-3\n",
    "\n",
    "    # Give a high reward if no wheels go off the track and \n",
    "    # the car is somewhere in between the track borders \n",
    "    if all_wheels_on_track and (0.5*track_width - distance_from_center) >= 0.05:\n",
    "        reward = 1.0\n",
    "\n",
    "    # Always return a float value\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 3: Prevent Zig-Zag in Time Trials__\n",
    "\n",
    "This example incentivizes the agent to follow the center line but penalizes with lower reward if it steers too much, which helps prevent zig-zag behavior. The agent learns to drive smoothly in the simulator and likely keeps the same behavior when deployed in the physical vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of penalize steering, which helps mitigate zig-zag behaviors\n",
    "    '''\n",
    "    \n",
    "    # Read input parameters\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    steering = abs(params['steering_angle']) # Only need the absolute steering angle\n",
    "\n",
    "    # Calculate 3 marks that are farther and father away from the center line\n",
    "    marker_1 = 0.1 * track_width\n",
    "    marker_2 = 0.25 * track_width\n",
    "    marker_3 = 0.5 * track_width\n",
    "\n",
    "    # Give higher reward if the car is closer to center line and vice versa\n",
    "    if distance_from_center <= marker_1:\n",
    "        reward = 1.0\n",
    "    elif distance_from_center <= marker_2:\n",
    "        reward = 0.5\n",
    "    elif distance_from_center <= marker_3:\n",
    "        reward = 0.1\n",
    "    else:\n",
    "        reward = 1e-3  # likely crashed/ close to off track\n",
    "\n",
    "    # Steering penality threshold, change the number based on your action space setting\n",
    "    ABS_STEERING_THRESHOLD = 15 \n",
    "\n",
    "    # Penalize reward if the car is steering too much\n",
    "    if steering > ABS_STEERING_THRESHOLD:\n",
    "        reward *= 0.8\n",
    "\n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you design your reward function to act like an incentive plan.<br> \n",
    "For example, to keep the agent staying as close to the center line as possible, the reward function could return a reward of 1.0 if the vehicle is within 3cm from the center, a reward of 0.5 if the agent is within 10cm and a reward of 0.001 (stands for zero for practical purposes) otherwise.<br>\n",
    "\n",
    "You can customize your reward function with relevant input parameters passed into the reward function. However, you should understand that if your reward function includes details specific to the training track (such as the shape of the track), your model might learn a policy which does not generalize to other tracks.\n",
    "\n",
    "Note that the model training process will try to optimize average total reward. Without taking this into account, your model might learn a policy which maximizes time spent accumulating reward (for example, by driving slowly or zig-zagging)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 4: Stay On One Lane without Crashing into Stationary Obstacles or Moving Vehicles__\n",
    "\n",
    "This reward function rewards the agent to stay between the track borders and penalizes the agent for getting too close to the next object in the front.<br> The agent can move from lane to lane to avoid crashes.<br> The total reward is a weighted sum of the reward and penalty.<br> The example gives more weight to the penalty term to focus more on safety by avoiding crashes.<br> You can play with different averaging weights to train the agent with different driving behaviors and to achieve different driving performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: This example is for the object avoidance race type and can be ignored for the time trail competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    '''\n",
    "    Example of rewarding the agent to stay inside two borders\n",
    "    and penalizing getting too close to the objects in front\n",
    "    '''\n",
    "\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    track_width = params['track_width']\n",
    "    objects_distance = params['objects_distance']\n",
    "    _, next_object_index = params['closest_objects']\n",
    "    objects_left_of_center = params['objects_left_of_center']\n",
    "    is_left_of_center = params['is_left_of_center']\n",
    "\n",
    "    # Initialize reward with a small number but not zero\n",
    "    # because zero means off-track or crashed\n",
    "    reward = 1e-3\n",
    "\n",
    "    # Reward if the agent stays inside the two borders of the track\n",
    "    if all_wheels_on_track and (0.5 * track_width - distance_from_center) >= 0.05:\n",
    "        reward_lane = 1.0\n",
    "    else:\n",
    "        reward_lane = 1e-3\n",
    "\n",
    "    # Penalize if the agent is too close to the next object\n",
    "    reward_avoid = 1.0\n",
    "\n",
    "    # Distance to the next object\n",
    "    distance_closest_object = objects_distance[next_object_index]\n",
    "    # Decide if the agent and the next object is on the same lane\n",
    "    is_same_lane = objects_left_of_center[next_object_index] == is_left_of_center\n",
    "    \n",
    "    if is_same_lane:\n",
    "        if 0.5 <= distance_closest_object < 0.8: \n",
    "            reward_avoid *= 0.5\n",
    "        elif 0.3 <= distance_closest_object < 0.5:\n",
    "            reward_avoid *= 0.2\n",
    "        elif distance_closest_object < 0.3:\n",
    "            reward_avoid = 1e-3 # Likely crashed\n",
    "\n",
    "    # Calculate reward by putting different weights on \n",
    "    # the two aspects above\n",
    "    reward += 1.0 * reward_lane + 4.0 * reward_avoid\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 AWS DeepRacer Reward Function Examples - Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on experiences of people at various different AWS DeepRacer events across the globe, we have collated a set of advanced Rewward functions which could help you achieve faster times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Source- [Github- scottpletcher/deepracer](https://github.com/scottpletcher/deepracer)__\n",
    "\n",
    "Selecting the top reward functions by the author. You can learn more by clicking on the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PurePursuit__\n",
    "\n",
    "Based on an acedemic paper from 1992 by R. Craig Coulter titled \"Implementation of the Pure Pursuit Tracking Algorithm\".\n",
    "\n",
    "When we drive a real car, we don't look out the side window and ensure we're a distance from the side of the road rather, we identify a point down the road and use that to orient ourselves.\n",
    "\n",
    "All the hyperparameters were set to default and training period was for about 4 hours\n",
    "\n",
    "__Note__: this function is not an exact replica of the one stated at the source as the parameter listing have changed over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "    \n",
    "    import math\n",
    "    \n",
    "    reward = 1e-3\n",
    "        \n",
    "    rabbit = [0,0]\n",
    "    pointing = [0,0]\n",
    "    \n",
    "    # Read input variables\n",
    "    waypoints = params['waypoints']\n",
    "    closest_waypoints = params['closest_waypoints']\n",
    "    heading = params['heading']\n",
    "    x=params['x']\n",
    "    y=params['y']\n",
    "    # Reward when yaw (car_orientation) is pointed to the next waypoint IN FRONT.\n",
    "    \n",
    "    # Find nearest waypoint coordinates\n",
    "    rabbit = waypoints[closest_waypoints[1]]\n",
    "    \n",
    "    radius = math.hypot(x - rabbit[0], y - rabbit[1])\n",
    "    \n",
    "    pointing[0] = x + (radius * math.cos(heading))\n",
    "    pointing[1] = y + (radius * math.sin(heading))\n",
    "    \n",
    "    vector_delta = math.hypot(pointing[0] - rabbit[0], pointing[1] - rabbit[1])\n",
    "    \n",
    "    # Max distance for pointing away will be the radius * 2\n",
    "    # Min distance means we are pointing directly at the next waypoint\n",
    "    # We can setup a reward that is a ratio to this max.\n",
    "        \n",
    "    if vector_delta == 0:\n",
    "        reward += 1\n",
    "    else:\n",
    "        reward += ( 1 - ( vector_delta / (radius * 2)))\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SelfMotivator__\n",
    "\n",
    "_With supervised learning, your model will only be as good as the ground truth you have to give it. With reinforcement learning, the model has the potential to become better than anything or anyone has ever done that thing._\n",
    "\n",
    "Trust in the reinforcement learning process to figure out the best way around the track.<br>\n",
    "The author decided to create a simple function that simply motivated the model to stay on the track and get around in as few steps as possible. \n",
    "\n",
    "Trained for about 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(params):\n",
    "\n",
    "    if params[\"all_wheels_on_track\"] and params[\"steps\"] > 0:\n",
    "        reward = ((params[\"progress\"] / params[\"steps\"]) * 100) + (params[\"speed\"]**2)\n",
    "    else:\n",
    "        reward = 0.01\n",
    "        \n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Source- [Medium- beSharp](https://medium.com/proud2becloud/deepracer-our-journey-to-the-top-ten-257ff69922e)__\n",
    "\n",
    "Selecting the top reward functions by the author. You can learn more by clicking on the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def reward_function(params):\n",
    "    \n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    steering = abs(params['steering_angle'])\n",
    "    direction_stearing=params['steering_angle']\n",
    "    speed = params['speed']\n",
    "    steps = params['steps']\n",
    "    progress = params['progress']\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    ABS_STEERING_THRESHOLD = 15\n",
    "    SPEED_TRESHOLD = 5\n",
    "    TOTAL_NUM_STEPS = 85\n",
    "    \n",
    "    # Read input variables\n",
    "    waypoints = params['waypoints']\n",
    "    closest_waypoints = params['closest_waypoints']\n",
    "    heading = params['heading']\n",
    "    \n",
    "    reward = 1.0\n",
    "        \n",
    "    if progress == 100:\n",
    "        reward += 100\n",
    "    \n",
    "    # Calculate the direction of the center line based on the closest waypoints\n",
    "    next_point = waypoints[closest_waypoints[1]]\n",
    "    prev_point = waypoints[closest_waypoints[0]]\n",
    "    \n",
    "    # Calculate the direction in radius, arctan2(dy, dx), the result is (-pi, pi) in radians\n",
    "    track_direction = math.atan2(next_point[1] - prev_point[1], next_point[0] - prev_point[0]) \n",
    "    \n",
    "    # Convert to degree\n",
    "    track_direction = math.degrees(track_direction)\n",
    "    \n",
    "    # Calculate the difference between the track direction and the heading direction of the car\n",
    "    direction_diff = abs(track_direction - heading)\n",
    "    \n",
    "    # Penalize the reward if the difference is too large\n",
    "    DIRECTION_THRESHOLD = 10.0\n",
    "    \n",
    "    malus=1\n",
    "    \n",
    "    if direction_diff > DIRECTION_THRESHOLD:\n",
    "        malus=1-(direction_diff/50)\n",
    "        if malus<0 or malus>1:\n",
    "            malus = 0\n",
    "        reward *= malus\n",
    "    \n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Source- [Medium- Sarah Lueck](https://medium.com/axel-springer-tech/how-to-win-aws-deepracer-ce15454f594a)__\n",
    "\n",
    "Selecting the top reward functions by the author. You can learn more by clicking on the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def reward_function(params):\n",
    "\n",
    "    # Read input parameters\n",
    "    track_width = params['track_width']\n",
    "    distance_from_center = params['distance_from_center']\n",
    "    all_wheels_on_track = params['all_wheels_on_track']\n",
    "    is_left_of_center = params['is_left_of_center']\n",
    "    steering_angle = params['steering_angle']\n",
    "    speed = params['speed']\n",
    "    \n",
    "    if is_left_of_center == True:\n",
    "        distance_from_center *= -1\n",
    "\n",
    "    # implementation of reward function for distance from center\n",
    "    reward = (1 / (math.sqrt(2 * math.pi * (track_width*2/15) ** 2)) * math.exp(-((\n",
    "            distance_from_center + track_width/20) ** 2 / (4 * track_width*2/15) ** 2))) *(track_width*1/3)\n",
    "    \n",
    "    if not all_wheels_on_track:\n",
    "        reward = 1e-3\n",
    "\n",
    "    # implementation of reward function for steering angle\n",
    "    STEERING_THRESHOLD = 14.4\n",
    "    \n",
    "    if abs(steering_angle) < STEERING_THRESHOLD:\n",
    "        steering_reward = math.sqrt(- (8 ** 2 + steering_angle ** 2) + math.sqrt(4 * 8 ** 2 * steering_angle ** 2 + (12 ** 2) ** 2) ) / 10\n",
    "    else:\n",
    "        steering_reward = 0\n",
    "\n",
    "    # aditional reward if the car is not steering too much\n",
    "    reward *= steering_reward\n",
    "\n",
    "    # reward for the car taking fast actions (speed is in m/s)\n",
    "    reward *= math.sin(speed/math.pi * 5/6)\n",
    "    \n",
    "    # same reward for going slow with greater steering angle then going fast straight ahead \n",
    "    reward *= math.sin(0.4949 * (0.475 * (speed - 1.5241) + 0.5111 * steering_angle ** 2))\n",
    "\n",
    "    return float(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Source- [Github- VilemR/AWS_DeepRacer](https://github.com/VilemR/AWS_DeepRacer)__\n",
    "\n",
    "Learn more by clicking on the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Additional Information and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convergence:__\n",
    "    \n",
    "Convergence in Machine learning describes a set of weights during supervised training, as a model begins to find (converge on) the values needed to produce the correct (trained) response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mbm-Lv5Un3Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mbm-Lv5Un3Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Action Space__\n",
    "\n",
    "For autonomous driving, the AWS DeepRacer vehicle receives input images streamed at 15 frames per second from the front camera. The raw input is downsized to 160x120 pixels in size and converted to grayscale images.\n",
    "\n",
    "Responding to an input observation, the vehicle reacts with a well-defined action of specific speed and steering angle. The actions are converted to low-level motor controls. The possible actions a vehicle can take is defined by an action space of the dimensions in speed and steering angle. An action space can be discrete or continuous. AWS DeepRacer uses a discrete action space.\n",
    "\n",
    "For a discrete action space of finite actions, the range is defined by the maximum speed and the absolute value of the maximum steering angles. The granularities define the number of speeds and steering angles the agent has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/a6q-safxklY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/a6q-safxklY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overfitting__\n",
    "\n",
    "Overfitting refers to a model that models the training data too well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bmV7LJd4DtQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/bmV7LJd4DtQ\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Log Analysis__\n",
    "\n",
    "One of the most involving aspect of learning with DeepRacer is observing the impact of changes to the training and identifying what can be improved to progress. Without the ability to see that training becomes walking in the dark and can be both frustrating and wasteful.\n",
    "\n",
    "Log analysis lets you read the training, evaluation or submission logs, load them into data series formats and process them to provide data in form of tables or plots.\n",
    "\n",
    "Learn more here [Introducing AWS DeepRacer Log Analysis](https://blog.deepracing.io/2020/03/30/introducing-aws-deepracer-log-analysis/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YSSdFnBZ5gc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YSSdFnBZ5gc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Waypoints__\n",
    "\n",
    "Map put your machine learning journey with waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xBpglYdmdyw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xBpglYdmdyw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variable future waypoints__\n",
    "\n",
    "Help your model keep its eye on the road with variable future waypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9hzPYLxd6Mw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/9hzPYLxd6Mw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __3.Hyperparameters__\n",
    "\n",
    "Hyperparameters are variables to control your reinforcement learning training. They can be tuned to optimize the training time and your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization technique used by AWS Deepracer is the PPO which stands for Proximal Policy Optimization.\n",
    "To know more about the PPO use this link [PPO/OpenAI](https://openai.com/blog/openai-baselines-ppo/#ppo)\n",
    "\n",
    "Looking into the various hyper parameters:\n",
    "\n",
    "__1. Gradient descent batch size__\n",
    "\n",
    "The number of recent vehicle experiences sampled at random from an experience buffer and used for updating the underlying deep-learning neural network weights. Random sampling helps reduce correlations inherent in the input data. Use a larger batch size to promote more stable and smooth updates to the neural network weights, but be aware of the possibility that the training may be longer or slower.\n",
    "\n",
    "The batch is a subset of an experience buffer that is composed of images captured by the camera mounted on the AWS DeepRacer vehicle and actions taken by the vehicle.\n",
    "\n",
    "\n",
    "__2. Number of epochs__\n",
    "\n",
    "The number of passes through the training data to update the neural network weights during gradient descent. The training data corresponds to random samples from the experience buffer. Use a larger number of epochs to promote more stable updates, but expect a slower training. When the batch size is small, you can use a smaller number of epochs.\n",
    "\n",
    "\n",
    "__3. Learning rate__\n",
    "\n",
    "During each update, a portion of the new weight can be from the gradient-descent (or ascent) contribution and the rest from the existing weight value. The learning rate controls how much a gradient-descent (or ascent) update contributes to the network weights. Use a higher learning rate to include more gradient-descent contributions for faster training, but be aware of the possibility that the expected reward may not converge if the learning rate is too large.\n",
    "\n",
    "\n",
    "__4. Entropy__\n",
    "\n",
    "The degree of uncertainty used to determine when to add randomness to the policy distribution. The added uncertainty helps the AWS DeepRacer vehicle explore the action space more broadly. A larger entropy value encourages the vehicle to explore the action space more thoroughly.\n",
    "\n",
    "\n",
    "__5. Discount factor__\n",
    "\n",
    "The discount factor determines how much of future rewards are discounted in calculating the reward at a given state as the averaged reward over all the future states. The discount factor of 0 means the current state is independent of future steps, whereas the discount factor 1 means that contributions from all of the future steps are included. With the discount factor of 0.9, the expected reward at a given step includes rewards from an order of 10 future steps. With the discount factor of 0.999, the expected reward includes rewards from an order of 1000 future steps.\n",
    "\n",
    "\n",
    "__6. Loss type__\n",
    "\n",
    "The type of the objective function to update the network weights. A good training algorithm should make incremental changes to the vehicle’s strategy so that it gradually transitions from taking random actions to taking strategic actions to increase reward. But if it makes too big a change then the training becomes unstable and the agent ends up not learning. The Huber and Mean squared error loss types behave similarly for small updates. But as the updates become larger, the Huber loss takes smaller increments compared to the Mean squared error loss. When you have convergence problems, use the Huber loss type. When convergence is good and you want to train faster, use the Mean squared error loss type.\n",
    "\n",
    "\n",
    "__7. Number of experience episodes between each policy-updating iteration__\n",
    "\n",
    "The size of the experience buffer used to draw training data from for learning policy network weights. An episode is a period in which the vehicle starts from a given starting point and ends up completing the track or going off the track. Different episodes can have different lengths. For simple reinforcement-learning problems, a small experience buffer may be sufficient and learning will be fast. For more complex problems which have more local maxima, a larger experience buffer is necessary to provide more uncorrelated data points. In this case, training will be slower but more stable. The recommended values are 10, 20 and 40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7NUdvqRhRtM\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/7NUdvqRhRtM\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
