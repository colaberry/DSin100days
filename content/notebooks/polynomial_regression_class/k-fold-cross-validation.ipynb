{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.805354Z",
     "start_time": "2019-10-07T19:41:02.233983Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict,train_test_split, KFold, ShuffleSplit\n",
    "\n",
    "\n",
    "%matplotlib notebook \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cross Validation\n",
    "\n",
    "### Train- Test Split\n",
    "\n",
    "In order to tune the model to find the structure in the data in the best possible fashion, we must use the existing data to identify settings for the model’s parameters that yield the best and most realistic predictive performance.\n",
    "\n",
    "Traditionally, this has been achieved by splitting the existing data into training and test sets. The training set is used\n",
    "to build and tune the model and the test set is used to estimate the model’s predictive performance. Modern approaches to model building split the data into multiple training and testing sets, which have been shown to often find more optimal tuning parameters and give a more accurate representation of the model’s predictive performance.\n",
    "\n",
    "To avoid over-fitting, we use a general model building approach that encompasses model tuning and model evaluation\n",
    "with the ultimate goal of finding the reproducible structure in the data. This approach entails splitting existing data into distinct sets for the purposes of tuning model parameters and evaluating model performance. The choice of data splitting method depends on characteristics of the existing data such as its size and structure.\n",
    "\n",
    "Below is an example of how test train split can be done in python\n",
    " \n",
    "take the code below and run it in the cell- \n",
    "\n",
    "```python \n",
    "\n",
    "\n",
    "#load boston dataset using boston_dataset.feature_names\n",
    "boston_dataset = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_data['MEDV'] = boston_dataset.target\n",
    "\n",
    "#create X and y as input and output vectors\n",
    "X = boston_data[['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'PTRATIO', 'B', 'LSTAT']]\n",
    "y = boston_data[['MEDV']]\n",
    "\n",
    "# Write code below to create train-test split\n",
    "#Use sklearn.modelselection. train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "X_train.head()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.808430Z",
     "start_time": "2019-10-07T19:41:02.806764Z"
    }
   },
   "outputs": [],
   "source": [
    "# run test train split code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, example we took the boston dataset and broke it down into a test set with 30% of the examples and 70% of the examples in the training set. This breakdown allows us to train the model on the training set and run it on the test set. One should never train the model on examples in the test set. \n",
    "\n",
    "What does that mean? \n",
    "\n",
    "It means that for example, in the context of what we have learned so far, we should never run the fit function on the test set data in linear regression. Just the predict function. \n",
    "\n",
    "We expect the test set metric scores, accuracy or mean squared error(in case of regression) to lower than than the training set accuracy score. This is because the test set may have data the model has not seen before.  For example, if your house price variation is say 10,000 dollars based on location and the test set has examples where that variation is far larger then the predicted value of the house price will be lower than the actual value in the test set. Hence, the mean squared error will be higher in the test set. \n",
    "\n",
    "## K - fold cross validation\n",
    "\n",
    "Now what if the test set of example that the train data does not contain at all? For example suppose you are training a model to fit the boston housing dataset and the test set has house prices out of the range of the the training set entirely. Also, suppose these are not outliers. Since we are doing a random train-test split this can easily happen. Also for smaller datasets, this can definitely happen where your model may miss some key examples that it can learn from due to the test train split. \n",
    "\n",
    "In such scenarios, we can use K-fold cross validation. K-fold cross validation is useful especially when \n",
    "\n",
    "- You have a small dataset and partation into test and train is not feasible<br>\n",
    "- you want to check how much variation is there in the model <br>\n",
    "\n",
    "So what is k fold cross validation- Rather than splitting the data into a single test train split, we split it into multiple sets. For example, we may choose to split the dataset into 5 sets. We set the first set as the validation set and train a model. Then we choose the second set as the validation set and train a model on that and so on. Finally, iterating through the sets, 5 models will have been trained and subsequently the accuracies and the error from the 5 validation sets will also be availaible. On this basis we can easily check if the variation major or minor. If for example, for the 5 models we have an $r^2$ score of (0.8,0.9,0.87,0.89,0.91) then the model variation is minor. Suppose in one of the models the $r^2$ score is really tiny like 0.1 then we know that there is something wrong with that specific set of training data. In such cases we can go back and check our training data to make sure that there are no errors. \n",
    "\n",
    "The benefit of k fold cross validation is that it allows us to build a model using all the data that we have and especially with models which can be trained quickly cross validation works well. \n",
    "\n",
    "\n",
    "\n",
    "to show this we will show the example of running K- fold cross validation on an examples of linear regression with the Shampoo dataset and the Boston housing dataset. \n",
    "\n",
    "# K fold cross - Shampoo sales dataset\n",
    "\n",
    "\n",
    "Run the code below to perform k fold cross validation on the shampoo sales dataset. (This original dataset is credited to Makridakis, Wheelwright, and Hyndman (1998))\n",
    "\n",
    "```Python\n",
    "# import the shampoo sales dataset \n",
    "shampoo_data = pd.read_csv(\"shampoo_sales.csv\")\n",
    "sales_full = np.array(shampoo_data[\"Sales\"].tolist()) \n",
    "sales_full_reshaped = sales_full.reshape(sales_full.shape[0], 1)\n",
    "x_values_full = np.arange(0, sales_full.shape[0])\n",
    "x_values_full_reshaped = x_values_full.reshape(x_values_full.shape[0],1)\n",
    "\n",
    "lr_cv = LinearRegression()\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "cross_val_score(lr_cv, x_values_full_reshaped, sales_full_reshaped , cv=cv, scoring=\"r2\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.816896Z",
     "start_time": "2019-10-07T19:41:02.809838Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy and run the code for k fold cross validation on shampoo dataset here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T18:16:56.661367Z",
     "start_time": "2019-10-07T18:16:56.655076Z"
    }
   },
   "source": [
    "\n",
    "Notice you did the model building a bit more differently here. Rather than the straight out linear regression we import cross_val_score from sklearn.model_selection and then run our model. Another important line to pay attention to is the definition of cv. Unlike the sklearn documentation we did not definite cv as a number. We used 'ShuffleSplit' to provide values for cv. This is done because without shuffling the data cross validation will not yield appropriate values. \n",
    "\n",
    "Question: What happens when you remove the shuffle split line from the code and run the cross validation code for 'cv=5' ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.822848Z",
     "start_time": "2019-10-07T19:41:02.817960Z"
    }
   },
   "outputs": [],
   "source": [
    "# run code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.855513Z",
     "start_time": "2019-10-07T19:41:02.823812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.79471507, -0.74214013, -0.01991722, -0.4458732 , -1.77163797])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# solution\n",
    "# import the shampoo sales dataset \n",
    "shampoo_data = pd.read_csv(\"shampoo_sales.csv\")\n",
    "sales_full = np.array(shampoo_data[\"Sales\"].tolist()) \n",
    "sales_full_reshaped = sales_full.reshape(sales_full.shape[0], 1)\n",
    "x_values_full = np.arange(0, sales_full.shape[0])\n",
    "x_values_full_reshaped = x_values_full.reshape(x_values_full.shape[0],1)\n",
    "\n",
    "lr_cv = LinearRegression()\n",
    "cross_val_score(lr_cv, x_values_full_reshaped, sales_full_reshaped , cv=5, scoring=\"r2\")\n",
    "\n",
    "# you should see negative values in the k fold accuracies since k fold is not shuffing the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually predict the values for a given model using the 'cross_val_predict' function. Rather than running 'cross_val_score' we run the predict function- \n",
    "\n",
    "\n",
    "```Python \n",
    "shampoo_data = pd.read_csv(\"shampoo_sales.csv\")\n",
    "sales_full = np.array(shampoo_data[\"Sales\"].tolist()) \n",
    "sales_full_reshaped = sales_full.reshape(sales_full.shape[0], 1)\n",
    "x_values_full = np.arange(0, sales_full.shape[0])\n",
    "x_values_full_reshaped = x_values_full.reshape(x_values_full.shape[0],1)\n",
    "\n",
    "lr_cv = LinearRegression()\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=1)\n",
    "cross_val_predict(lr_cv, x_values_full_reshaped, sales_full_reshaped , cv=cv, scoring=\"r2\")\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:45:38.921265Z",
     "start_time": "2019-10-07T19:45:38.912495Z"
    }
   },
   "outputs": [],
   "source": [
    "# run cross val predict code here \n",
    "shampoo_data = pd.read_csv(\"shampoo_sales.csv\")\n",
    "\n",
    "sales_full = np.array(shampoo_data[\"Sales\"].tolist()) \n",
    "sales_full = np.ndarray.flatten(sales_full)\n",
    "sales_full_reshaped = sales_full.reshape(sales_full.shape[0], 1)\n",
    "\n",
    "x_values_full = np.arange(0, sales_full.shape[0])\n",
    "x_values_full = np.ndarray.flatten(x_values_full)\n",
    "x_values_full_reshaped = x_values_full.reshape(x_values_full.shape[0],1)\n",
    "\n",
    "lr_pr = LinearRegression()\n",
    "cross_val_predict(lr_pr, x_values_full_reshaped, sales_full_reshaped, cv=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Cross validate predict does not work with 'ShuffleSplit'  so make sure that you shuffle your dataset before you cross validate predict it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do cross validation manually. Sklearn allows you to th break up the data using a function called 'KFold' \n",
    "\n",
    "run the code below to break up the data and get individual sets. \n",
    "\n",
    "```Python\n",
    "\n",
    "# run the code above for generating row incidices from k fold cross validation \n",
    "\n",
    "kf = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "kf.get_n_splits(shampoo_data.values)\n",
    "\n",
    "for counter, indices in enumerate(kf.split(shampoo_data.values)):\n",
    "    train_index ,validation_index= indices\n",
    "    print(\"cross validation iteration {} \\nvalidation indices: \\n {} \\ntrain indices: \\n {}\".format(counter, validation_index, train_index))\n",
    "    \n",
    "```\n",
    "**Output**\n",
    "```python \n",
    "cross validation iteration 0 \n",
    "validation indices: \n",
    " [ 3 17 19 21 23 26 27 28 29 30 33 34] \n",
    "train indices: \n",
    " [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 18 20 22 24 25 31 32 35]\n",
    "cross validation iteration 1 \n",
    "validation indices: \n",
    " [ 2  4  6 10 14 18 20 22 24 25 31 32] \n",
    "train indices: \n",
    " [ 0  1  3  5  7  8  9 11 12 13 15 16 17 19 21 23 26 27 28 29 30 33 34 35]\n",
    "cross validation iteration 2 \n",
    "validation indices: \n",
    " [ 0  1  5  7  8  9 11 12 13 15 16 35] \n",
    "train indices: \n",
    " [ 2  3  4  6 10 14 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34]\n",
    "\n",
    "````\n",
    "\n",
    "when you print the test and the train index fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.871379Z",
     "start_time": "2019-10-07T19:41:02.869151Z"
    }
   },
   "outputs": [],
   "source": [
    "# run the code above for generating row incidices from k fold cross validation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we did above is generate the row indices using which we can preform cross validation. That is what is next expected of you- \n",
    "\n",
    "Question: using the code above, generate the validation sets and training sets for the shampoo datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-07T19:41:02.888085Z",
     "start_time": "2019-10-07T19:41:02.872751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: \n",
      "    Month  Sales\n",
      "0   1-01  266.0\n",
      "1   1-02  145.9\n",
      "2   1-03  183.1\n",
      "4   1-05  180.3\n",
      "5   1-06  168.5\n",
      "6   1-07  231.8\n",
      "7   1-08  224.5\n",
      "8   1-09  192.8\n",
      "9   1-10  122.9\n",
      "10  1-11  336.5\n",
      "11  1-12  185.9\n",
      "12  2-01  194.3\n",
      "13  2-02  149.5\n",
      "14  2-03  210.1\n",
      "15  2-04  273.3\n",
      "16  2-05  191.4\n",
      "18  2-07  226.0\n",
      "20  2-09  289.9\n",
      "22  2-11  264.5\n",
      "24  3-01  339.7\n",
      "25  3-02  440.4\n",
      "31  3-08  407.6\n",
      "32  3-09  682.0\n",
      "35  3-12  646.9 \n",
      " validation rows \n",
      "    Month  Sales\n",
      "3   1-04  119.3\n",
      "17  2-06  287.0\n",
      "19  2-08  303.6\n",
      "21  2-10  421.6\n",
      "23  2-12  342.3\n",
      "26  3-03  315.9\n",
      "27  3-04  439.3\n",
      "28  3-05  401.3\n",
      "29  3-06  437.4\n",
      "30  3-07  575.5\n",
      "33  3-10  475.3\n",
      "34  3-11  581.3\n",
      "train rows: \n",
      "    Month  Sales\n",
      "0   1-01  266.0\n",
      "1   1-02  145.9\n",
      "3   1-04  119.3\n",
      "5   1-06  168.5\n",
      "7   1-08  224.5\n",
      "8   1-09  192.8\n",
      "9   1-10  122.9\n",
      "11  1-12  185.9\n",
      "12  2-01  194.3\n",
      "13  2-02  149.5\n",
      "15  2-04  273.3\n",
      "16  2-05  191.4\n",
      "17  2-06  287.0\n",
      "19  2-08  303.6\n",
      "21  2-10  421.6\n",
      "23  2-12  342.3\n",
      "26  3-03  315.9\n",
      "27  3-04  439.3\n",
      "28  3-05  401.3\n",
      "29  3-06  437.4\n",
      "30  3-07  575.5\n",
      "33  3-10  475.3\n",
      "34  3-11  581.3\n",
      "35  3-12  646.9 \n",
      " validation rows \n",
      "    Month  Sales\n",
      "2   1-03  183.1\n",
      "4   1-05  180.3\n",
      "6   1-07  231.8\n",
      "10  1-11  336.5\n",
      "14  2-03  210.1\n",
      "18  2-07  226.0\n",
      "20  2-09  289.9\n",
      "22  2-11  264.5\n",
      "24  3-01  339.7\n",
      "25  3-02  440.4\n",
      "31  3-08  407.6\n",
      "32  3-09  682.0\n",
      "train rows: \n",
      "    Month  Sales\n",
      "2   1-03  183.1\n",
      "3   1-04  119.3\n",
      "4   1-05  180.3\n",
      "6   1-07  231.8\n",
      "10  1-11  336.5\n",
      "14  2-03  210.1\n",
      "17  2-06  287.0\n",
      "18  2-07  226.0\n",
      "19  2-08  303.6\n",
      "20  2-09  289.9\n",
      "21  2-10  421.6\n",
      "22  2-11  264.5\n",
      "23  2-12  342.3\n",
      "24  3-01  339.7\n",
      "25  3-02  440.4\n",
      "26  3-03  315.9\n",
      "27  3-04  439.3\n",
      "28  3-05  401.3\n",
      "29  3-06  437.4\n",
      "30  3-07  575.5\n",
      "31  3-08  407.6\n",
      "32  3-09  682.0\n",
      "33  3-10  475.3\n",
      "34  3-11  581.3 \n",
      " validation rows \n",
      "    Month  Sales\n",
      "0   1-01  266.0\n",
      "1   1-02  145.9\n",
      "5   1-06  168.5\n",
      "7   1-08  224.5\n",
      "8   1-09  192.8\n",
      "9   1-10  122.9\n",
      "11  1-12  185.9\n",
      "12  2-01  194.3\n",
      "13  2-02  149.5\n",
      "15  2-04  273.3\n",
      "16  2-05  191.4\n",
      "35  3-12  646.9\n"
     ]
    }
   ],
   "source": [
    "# solution \n",
    "kf = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "kf.get_n_splits(shampoo_data.values)\n",
    "\n",
    "for counter, indices in enumerate(kf.split(shampoo_data.values)):\n",
    "    train_index ,validation_index= indices\n",
    "    \n",
    "    train_rows = shampoo_data.iloc[train_index]\n",
    "    validation_rows = shampoo_data.iloc[validation_index]\n",
    "    \n",
    "    print(\"train rows: \\n {} \\n validation rows \\n {}\".format(train_rows, validation_rows))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we saw how to do cross validation with sklearn in two ways. There are other functions that will be useful to learn  when using cross validation- \n",
    "\n",
    "some function worth exploring are - \n",
    "\n",
    "1) LogisticRegressionCV where you can logistic regression with cross validation. \n",
    "2) There are different types of cross validation that you can do for example-\n",
    "    - GroupKFold\n",
    "    - StratifiedKFold \n",
    "  \n",
    "Now that you have a basic idea of how to implement regular cross validation, it will be easy to look at the sklearn documentation so that you can better understand how well to implement other cross validation methods. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
