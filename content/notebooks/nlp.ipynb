{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "s1",
     "content",
     "l1"
    ]
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "## Natural Language Processing (NLP)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "NLP involves computing algorithms to mathematically analyze natural human languages. To understand NLP, we need to analyze how humans comprehend language. NLP is useful for Information\n",
    "retrieval, sentiment analysis, Information Extraction, Machine Translation, Prediction of likely text in search boxes. It is also used in analyzing post speech recognition texts. \n",
    "\n",
    "\n",
    "<img src=\"../images/NLP.png\" style=\"width: 350px;\">\n",
    "\n",
    "Here are various operations of NLP that we are going to learn:\n",
    "\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Stemming and lemmatization by recognition of root words.\n",
    "- Part-of-speech tagging\n",
    "- Named entity recognition\n",
    "\n",
    "<img src=\"../images/nlp-flow.png\" style=\"width: 350px;\">\n",
    "\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "Tokenization is a process of understanding a sentence or a phrase by splitting them into substrings, words and punctuation.\n",
    "\n",
    "```python\n",
    ">>> from nltk.tokenize import word_tokenize\n",
    ">>> text = '''All extraterrestrial activity today is governed by a 50-year-old treaty drafted at the height of the Cold War. Will governments around the world agree on an update before the final frontier becomes the Wild West?'''\n",
    ">>> word_tokenize(text)\n",
    "['All', 'extraterrestrial', 'activity', 'today', 'is', 'governed', 'by', 'a', '50-year-old', 'treaty', 'drafted', 'at', 'the', 'height', 'of', 'the', 'Cold', 'War', '.', 'Will', 'governments', 'around', 'the', 'world', 'agree', 'on', 'an', 'update', 'before', 'the', 'final', 'frontier', 'becomes', 'the', 'Wild', 'West', '?']\n",
    "```\n",
    "\n",
    "Similar to tokenizing words, we can tokenize sentences to split the text into the sentence levels.\n",
    "\n",
    "```python\n",
    ">>> from nltk.tokenize import sent_tokenize\n",
    ">>> sent_tokenize(text)\n",
    "['All extraterrestrial activity today is governed by a 50-year-old treaty drafted at the height of the Cold War.', 'Will governments around the world agree on an update before the final frontier becomes the Wild West?']\n",
    "```\n",
    "\n",
    "We can see that the text has been split up into multiple sentences. Therefore tokenizing is the first step in the analysis. \n",
    "\n",
    "### Stop Words\n",
    "\n",
    "Most of the times we are interested in the useful words out a sentence either to recognize the text or to extract features from it. Towards that end, commonly occurring words such as 'the', 'is', 'and' would not be useful to us. Such words are called stop words. They can be listed in the nltk toolkit. You can list the stop words:\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    ">>> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "Given the text from the wall street journal, tokenize the text at the word level and remove the stop words.\n",
    "\n",
    "- Assign the list to the variable, word_features and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "tags": [
     "s1",
     "ce",
     "l1"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wsj_text = '''All extraterrestrial activity today is governed by a 50-year-old treaty drafted at the height of the Cold War. Will governments around the world agree on an update before the final frontier becomes the Wild West?'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "s1",
     "l1",
     "hint"
    ]
   },
   "source": [
    "<p>use [x for x in list if x not in ] to filter data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "s1",
     "l1",
     "ans"
    ]
   },
   "outputs": [],
   "source": [
    "wsj_tokens = word_tokenize(wsj_text)\n",
    "word_features = [word for word in wsj_tokens if word not in stopwords.words('english')]\n",
    "print(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "s1",
     "hid",
     "l1"
    ]
   },
   "outputs": [],
   "source": [
    "ref_tmp_var = False\n",
    "\n",
    "\n",
    "try:\n",
    "    ref_assert_var = False\n",
    "    import numpy as np\n",
    "    \n",
    "    wsj_tokens_ = word_tokenize(wsj_text)\n",
    "    word_features_ = [word for word in wsj_tokens_ if word not in stopwords.words('english')]\n",
    "    \n",
    "    if np.all(word_features == word_features_):\n",
    "      ref_assert_var = True\n",
    "    else:\n",
    "      ref_assert_var = False\n",
    "    \n",
    "except Exception:\n",
    "    print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        ref_tmp_var = True\n",
    "    else:\n",
    "        print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "\n",
    "\n",
    "assert ref_tmp_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l2",
     "content",
     "s2"
    ]
   },
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "## Stemming\n",
    "\n",
    "### Stemming\n",
    "\n",
    "There is more cleaning we have to often to do extract features from a text before we can begin to use them. Removing stop words was the first step but that isnt enough. We see that the features extracted from the wall street journal text were:\n",
    "```python\n",
    "['All', 'extraterrestrial', 'activity', 'today', 'governed', '50-year-old', 'treaty', 'drafted', 'height', 'Cold', 'War', '.', 'Will', 'governments', 'around', 'world', 'agree', 'update', 'final', 'frontier', 'becomes', 'Wild', 'West', '?']\n",
    "```\n",
    "\n",
    "Words such as 'governed, drafted' are past tenses and the word 'becomes' is a plural word. Consider a scenario where another set of word features from a different class contains words such as \"governs, drafts and became\". These words refer to the same meaning but are regarded as different features. We need a way to map such word forms into their root word. That is possible by a word stemmer. There are many word stemmers available in nltk. Import the Porter Stemmer as:\n",
    "\n",
    "```python\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "```\n",
    "\n",
    "Instantiate a stemmer and run it on the wsj text. Note that the stemmer works at the word level and hence the stemmer needs to be run on every single word after removing the stopwords.\n",
    "\n",
    "```python\n",
    "stemmer = PorterStemmer()\n",
    "root_words = [stemmer.stem(word) for word in word_features]\n",
    "print(root_words)\n",
    "['all', 'extraterrestri', 'activ', 'today', 'govern', '50-year-old', 'treati', 'draft', 'height', 'cold', 'war', '.', 'will', 'govern', 'around', 'world', 'agre', 'updat', 'final', 'frontier', 'becom', 'wild', 'west', '?']\n",
    "```\n",
    "\n",
    "The root words need not be the same as the noun form of the word or even a word in the dictionary. It is a word feature used to associate various forms of the word to itself. \n",
    "\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "A better stemmer is the Snowball Stemmer. It is available in the nltk.stem.snowball module. The stemmer works for many languages. Hence the SnowballStemmer takes in the language \"english\" as an argument.\n",
    "\n",
    "- Extract root words using the SnowballStemmer from the word_features and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l2",
     "ce",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l2",
     "s2",
     "hint"
    ]
   },
   "source": [
    "<p>works similar to PorterStemmer()</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l2",
     "s2",
     "ans"
    ]
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "root_words = [stemmer.stem(word) for word in word_features]\n",
    "print(root_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l2",
     "hid",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "ref_tmp_var = False\n",
    "\n",
    "\n",
    "try:\n",
    "    ref_assert_var = False\n",
    "    stemmer_ = SnowballStemmer(\"english\")\n",
    "    root_words_ = [stemmer_.stem(word) for word in word_features]\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if np.all(root_words == root_words_):\n",
    "      ref_assert_var = True\n",
    "    else:\n",
    "      ref_assert_var = False\n",
    "    \n",
    "except Exception:\n",
    "    print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        ref_tmp_var = True\n",
    "    else:\n",
    "        print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "\n",
    "\n",
    "assert ref_tmp_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l3",
     "s3",
     "content"
    ]
   },
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "## Lemmatization\n",
    "\n",
    "Lemma is defined in the google dictionary as \"a heading indicating the subject or argument of a literary composition, an annotation, or a dictionary entry\". Lemmatization is the algorithmic method of determining the 'lemma' of a word form. The only difference between stemming is that it involves contextual understanding of a word. Therefore, Lemmataization works based on the parts-of-speech of the word. Unlike stemming, lemmatization yields real words in the dictionary. For example, lemmatizing the word \"governed\" with POS as verb and noun would yield different lemmas:\n",
    "\n",
    "```python\n",
    ">>> lemmatizer.lemmatize(\"governed\", pos=\"v\")\n",
    "'govern'\n",
    "\n",
    ">>> lemmatizer.lemmatize(\"governed\", pos=\"n\")\n",
    "'governed'\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "- Lemmatize each word with pos as verb in the word_features list and assign it to the list, lemmas.\n",
    "- print out the list, lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l3",
     "s3",
     "ce"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l3",
     "s3",
     "hint"
    ]
   },
   "source": [
    "<p>Use it similar to stemmers.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l3",
     "s3",
     "ans"
    ]
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=\"v\") for word in word_features]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l3",
     "s3",
     "hid"
    ]
   },
   "outputs": [],
   "source": [
    "ref_tmp_var = False\n",
    "\n",
    "\n",
    "try:\n",
    "    ref_assert_var = False\n",
    "    import numpy as np\n",
    "    \n",
    "    lemmatizer_ = WordNetLemmatizer()\n",
    "    lemmas_ = [lemmatizer_.lemmatize(word, pos=\"v\") for word in word_features]\n",
    "    \n",
    "    if np.all(lemmas == lemmas_):\n",
    "      ref_assert_var = True\n",
    "    else:\n",
    "      ref_assert_var = False\n",
    "    \n",
    "except Exception:\n",
    "    print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        ref_tmp_var = True\n",
    "    else:\n",
    "        print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "\n",
    "\n",
    "assert ref_tmp_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l4",
     "s4",
     "content"
    ]
   },
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "## Parts-Of-Speech Tagging\n",
    "\n",
    "We learnt that the stemming generates root word forms but not real words. The lemmatization does generate root words given the POS. Since large parts of text, it would be impractical to input POS for every single word. There must be a mechanism where we can generate POS given the structure of the sentence. This can be done with a POS tagger. POS taggers look at the context, sentence structure and with Machine Learning algorithms generate the best guesses for the parts-of-speech. This is performed in two steps:\n",
    "\n",
    "### Tokenizing the Words\n",
    "\n",
    "The words need to be split into 'tokens' or individual words, symbols and other textual elements that exist in the dictionary. This is done with pos_tag in nltk:\n",
    "\n",
    "```python\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wsj_tokens = word_tokenize(wsj_text)\n",
    "```\n",
    "\n",
    "### POS Tagging\n",
    "\n",
    "Tagging POS is done by running pos_tag imported from nltk.tokenize module.\n",
    "\n",
    "```python\n",
    "wsj_pos_tokens = pos_tag(wsj_tokens)\n",
    ">>> wsj_pos_tokens\n",
    "[('All', 'DT'), ('extraterrestrial', 'JJ'), ('activity', 'NN'), ('today', 'NN'), ('is', 'VBZ'), ('governed', 'VBN'), ('by', 'IN'), ('a', 'DT'), ('50-year-old', 'JJ'), ('treaty', 'NN'), ('drafted', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('height', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Cold', 'NNP'), ('War', 'NNP'), ('.', '.'), ('Will', 'MD'), ('governments', 'NNS'), ('around', 'IN'), ('the', 'DT'), ('world', 'NN'), ('agree', 'NN'), ('on', 'IN'), ('an', 'DT'), ('update', 'NN'), ('before', 'IN'), ('the', 'DT'), ('final', 'JJ'), ('frontier', 'NN'), ('becomes', 'VBZ'), ('the', 'DT'), ('Wild', 'NNP'), ('West', 'NNP'), ('?', '.')]\n",
    "```\n",
    "\n",
    "The pos_tag generates list of tuples of word, token tuples.\n",
    "\n",
    "<img src=\"../images/POS_tagging.png\" style=\"width: 1000px;\">\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "After performing POS tagging, build out a dictionary that has a POS type as key and the list of words as their value. \n",
    "\n",
    "- Assign the dictionary to the variable, pos_word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l4",
     "s4",
     "ce"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wsj_tokens = word_tokenize(wsj_text)\n",
    "wsj_pos = pos_tag(wsj_tokens)\n",
    "\n",
    "# Determine the (type2, words) pairs\n",
    "pos_word_map = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "s4",
     "l4",
     "hint"
    ]
   },
   "source": [
    "<p>check if the type already exists in the dictionary and then append to the list. If it doesn't then assign a list to the key, type.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "s4",
     "l4",
     "ans"
    ]
   },
   "outputs": [],
   "source": [
    "pos_word_map = {}\n",
    "\n",
    "for (word, type2) in wsj_pos:\n",
    "    if type2 in pos_word_map.keys():\n",
    "        pos_word_map[type2].append(word)\n",
    "    else:\n",
    "        pos_word_map[type2] = [word]\n",
    " \n",
    "print(pos_word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "s4",
     "hid",
     "l4"
    ]
   },
   "outputs": [],
   "source": [
    "ref_tmp_var = False\n",
    "\n",
    "\n",
    "try:\n",
    "    ref_assert_var = False\n",
    "    pos_word_map_ = {}\n",
    "    \n",
    "    for (word, type2) in wsj_pos:\n",
    "        if type2 in pos_word_map_.keys():\n",
    "            pos_word_map_[type2].append(word)\n",
    "        else:\n",
    "            pos_word_map_[type2] = [word]\n",
    "    \n",
    "    if pos_word_map == pos_word_map_:\n",
    "      ref_assert_var = True\n",
    "    else:\n",
    "      ref_assert_var = False\n",
    "    \n",
    "except Exception:\n",
    "    print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        ref_tmp_var = True\n",
    "    else:\n",
    "        print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "\n",
    "\n",
    "assert ref_tmp_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l5",
     "content",
     "s5"
    ]
   },
   "source": [
    "<br/><br/><br/>\n",
    "## Information Retrieval(IR)\n",
    "\n",
    "Information retrieval (IR) is the study of extracting relevant information from a corpus of information sources. In the context of NLP, it involves identifying information through search such as recognizing entities that exist in the textual information. \n",
    "\n",
    "Wordcloud is one of the many ways to quickly see what a text is all about. It projects the importance or frequency of the words used in a text. Bigger the font, higher the frequency in which the word was used. Below is a wordcloud formed from all job descriptions for a big data engineer posted on stackoverflow.com.\n",
    "\n",
    "<img src=\"../images/cloud-bigdataengineer.png\" style=\"width: 700px;\">\n",
    "\n",
    "<br>\n",
    "'Named Entity Recognition' (NER) is another form of information retrieval. It classifies the named entities in the text as pre-determined categories such as name of person, name of organization etc. nltk has feature to perform NER.\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "wsj_text2 = \"CEO Michael McGarry of PPG Industries, the world’s largest paint maker, is pursuing Dutch rival Akzo Nobel, in one of the boldest trans-Atlantic bids in recent memory.\"\n",
    "\n",
    "ne_chunk(pos_tag(word_tokenize(wsj_text2)))\n",
    "Tree('S', [Tree('ORGANIZATION', [('CEO', 'NN')]), Tree('PERSON', [('Michael', 'NNP'), ('McGarry', 'NNP')]), ('of', 'IN'), Tree('ORGANIZATION', [('PPG', 'NNP'), ('Industries', 'NNPS')]), (',', ','), ('the', 'DT'), ('world’s', 'NN'), ('largest', 'JJS'), ('paint', 'NN'), ('maker', 'NN'), (',', ','), ('is', 'VBZ'), ('pursuing', 'VBG'), Tree('GPE', [('Dutch', 'NNP')]), ('rival', 'NN'), Tree('PERSON', [('Akzo', 'NNP'), ('Nobel', 'NNP')]), (',', ','), ('in', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('boldest', 'JJS'), ('trans-Atlantic', 'JJ'), ('bids', 'NNS'), ('in', 'IN'), ('recent', 'JJ'), ('memory', 'NN'), ('.', '.')])\n",
    "```\n",
    "\n",
    "This is very useful as the NER is able to identify that 'Michael', 'Akzo' is a 'Person' and that CEO falls under 'Organization' category.\n",
    "\n",
    "\n",
    "## Exercise:\n",
    "\n",
    "The tree output is a data structure that is returned by ne_chunk.\n",
    "\n",
    "- Input the leaves to the variable, ner_leaves. Use .leaves() function.\n",
    "- Print out the ner_leaves and analyze the information. What does it contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l5",
     "ce",
     "s5"
    ]
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "wsj_text2 = \"CEO Michael McGarry of PPG Industries, the world’s largest paint maker, is pursuing Dutch rival Akzo Nobel, in one of the boldest trans-Atlantic bids in recent memory.\"\n",
    "ner_info = ne_chunk(pos_tag(word_tokenize(wsj_text2)))\n",
    "\n",
    "# What are the leaves?\n",
    "\n",
    "ner_leaves = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l5",
     "s5",
     "hint"
    ]
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l5",
     "s5",
     "ans"
    ]
   },
   "outputs": [],
   "source": [
    "ner_leaves = ner_info.leaves()\n",
    "print(ner_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "hid",
     "l5",
     "s5"
    ]
   },
   "outputs": [],
   "source": [
    "ref_tmp_var = False\n",
    "\n",
    "\n",
    "try:\n",
    "    ref_assert_var = False\n",
    "    import numpy as np\n",
    "    \n",
    "    ner_leaves_ = ner_info.leaves()\n",
    "    \n",
    "    if np.all(ner_leaves == ner_leaves_):\n",
    "      ref_assert_var = True\n",
    "    else:\n",
    "      ref_assert_var = False\n",
    "    \n",
    "except Exception:\n",
    "    print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "else:\n",
    "    if ref_assert_var:\n",
    "        ref_tmp_var = True\n",
    "    else:\n",
    "        print('Please follow the instructions given and use the same variables provided in the instructions.')\n",
    "\n",
    "\n",
    "assert ref_tmp_var"
   ]
  }
 ],
 "metadata": {
  "executed_sections": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "rf_version": 1,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
