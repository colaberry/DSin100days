{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab - Naives Bayes Classifier\n",
    "\n",
    "In this lab you will be implementing Naive-Bayes Classifer through sklearn and visualizaing the results. We will be going to back to the seeds dataset and following the same steps.\n",
    "\n",
    "The seeds dataset contains physical information about three types of seeds. There are multiple features for each seeds and more information about the features can be found [here]('https://archive.ics.uci.edu/ml/datasets/seeds'). \n",
    "\n",
    "We have renamed the features as: \n",
    "1. area: area A,\n",
    "2. perimeter: perimeter P,\n",
    "3. compactness: compactness C = 4*pi*A/P^2,\n",
    "4. kernel_length: length of kernel,\n",
    "5. kernel_width: width of kernel,\n",
    "6. asymmetry_coefficient: asymmetry coefficient\n",
    "7. kg_length: length of kernel groove.\n",
    "8. class: Type of seeds [Kama, rosa, Canadian](In the same order) \n",
    "\n",
    "The source of the above data dictionary is the original page on UCI Machine Learning repositry, it can be found  [here]('https://archive.ics.uci.edu/ml/datasets/seeds'). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib \n",
    "font_specs = {'size':15}\n",
    "matplotlib.rc('font', **font_specs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"https://raw.githubusercontent.com/colaberry/DSin100days/master/data/seeds_dataset.csv\"\n",
    "column_names = ['area',\n",
    "                'perimeter',\n",
    "                'compactness',\n",
    "                'kernel_length',\n",
    "                'kernel_width',\n",
    "                'asymmetry_coefficient',\n",
    "                'kg_length',\n",
    "                'class']\n",
    "# Read the dataset using read dataset, you have been given the location, you will need to specify \n",
    "# the names of the headers and set the header to none\n",
    "seeds_df = \"####\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to seperate the features and the target. Remember the target is the 'class' column\n",
    "X = \"####\"\n",
    "y = \"####\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a pairplot to visualize the relationship between all the features.\n",
    "\"####\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose only two features 'compactness' and 'kg_length' to keep. There are two ways to do this\n",
    "# either drop the rest of the features, or just select these two from the data frame and keep them \n",
    "X_subset = \"####\"\n",
    "\n",
    "# Run a pair plot to see the data distribution looks like for them.\n",
    "\"####\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next you need to do test train split. Set the train_size to 0.7, random_state to 0 and shuffle to true.\n",
    "train_x, test_x , train_y, test_y  = \"####\"\n",
    "train_shape = \"####\"\n",
    "test_shape = \"####\"\n",
    "\n",
    "print(\"Shape of the training set is {}\".format(train_shape))\n",
    "print(\"Shape of the testing set is {}\".format(test_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of the training set is (147, 2) <br>\n",
    "Shape of the testing set is (63, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First you need to run gaussian naive bayes, you will need to set the multilcass option to one over rest. \n",
    "# Refer to the sklearn documentation on how to do that. \n",
    "gaussian_nb = \"####\"\n",
    "\n",
    "# Run fit on the training set then run predict on the test set \n",
    "preds = \"####\"\n",
    "ovr_acc =\"####\"\n",
    "\n",
    "print(\"Accuracy for the gaussian naive bayes is {:.2f}%\".format(ovr_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for the gaussian naive bayes is 92.06%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to plot \n",
    "def to_3d(x,y,plot_step=0.01): \n",
    "   \n",
    "\n",
    "    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    return xx, yy \n",
    "\n",
    "def plot_decision_surface(xx,yy,Z): \n",
    "    \n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "    return cs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are taking the 2d dataset we have and generating a grid of points\n",
    "# we will run predict on the enitre grid of points.\n",
    "xx, yy = \"####\"\n",
    "\n",
    "# We need to run predict on the entire grid of points. You need run \n",
    "# unravel the set of xx and yy. You can do that by using the a.ravel() function\n",
    "# where a is a numpy array. Furthermore you need to concate the unraveled objects \n",
    "# by using np.c_ in the following way np.c_[xx_unraveled, yy_unraveled ]. \n",
    "# Use gaussian naive bayes to predict the correct class\n",
    "Z = \"####\"\n",
    "\n",
    "# You need to reshape Z to the same shape of xx or yy\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Specify a color map for the original points.\n",
    "colors = ['yellow','black', 'orange']\n",
    "cmap= ListedColormap(colors)\n",
    "\n",
    "# Specify a figure size.\n",
    "fig= plt.figure(figsize=(10,10))\n",
    "\n",
    "# Plot the decision surface\n",
    "_ = plot_decision_surface(xx,yy, Z)\n",
    "\n",
    "# Plot the original points\n",
    "plt.scatter(\"####\", \"####\", c=\"####\",cmap=\"####\" )\n",
    "plt.xlim([0.7, 1.0])\n",
    "plt.xlabel('Compactness')\n",
    "plt.ylabel('Kernel Groove length (cm)')\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the naive bayes algorithm is giving us a better fit than the logistic regression fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
