{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Projects \n",
    "\n",
    "This document is there to help you sturcture your data science project. The flow of this guide will help you craft a data science project quickly and pay attention to some of the key elements of problem solving.\n",
    "\n",
    "When you are working on a data science project, these are some of the key parts that you need to work on: \n",
    "\n",
    "- Problem Statement\n",
    "- Data Acquisition\n",
    "- Data Dictionary\n",
    "- Feature extraction\n",
    "- Data Cleaning  \n",
    "- EDA and Data Visualization\n",
    "- Deriving Key insights from EDA\n",
    "- Model building\n",
    "- Evaluation \n",
    "- Deriving Key Insights from model\n",
    "- Exporting the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Problem statement\n",
    "\n",
    "You need clearly define the problem that you are solving and what is the final outcome that you hope to achieve. Typically when you are working with simple datasets like the UCI machine learning repository dataset, then your problem statement is decided by your dataset. However, the entire process of problem definition involves translating business goals to analysis goals. \n",
    "\n",
    "Ideally, you should choose a specific domain, choose a problem to solve then find data to solve the problem. In most cases, exact data may not be available; then you can look at various data sources and combine data to get the required dataset. In practice, however, you may have decided on what problem to solve based on what data is available and accessible to you. So you may have to go in the opposite direction where you need to decide on a dataset and then decide what questions I can ask of it and then attack the problem. \n",
    "\n",
    "Either way, keep in mind that in the process, you need to learn about different domains.\n",
    "\n",
    "- [ ] Clearly state your data source.\n",
    "- [ ] What type of data do you have? \n",
    "    - Structured/Unstructured.\n",
    "- [ ] What are you predicting?\n",
    "- [ ] What are your features? \n",
    "- [ ] What is your target? \n",
    "- [ ] What type of problem is it? \n",
    "    - [ ] Supervised/Unsupervised? \n",
    "    - [ ] Classification/Regression? \n",
    "- [ ] If you have to combine features to define the target, discuss that here.\n",
    "- [ ] Do you need to combine multiple data sources? \n",
    "- [ ] Articulate the final outcome that you hope to achieve with this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Acquisition\n",
    "\n",
    "In this section you will talk about how you found the data. \n",
    "\n",
    "- [ ] Make sure that you mention the source of your data.\n",
    "- [ ] If you are using multiple datasets, make sure you mention the source of all of those datasets \n",
    "and mention why you are using the datsets \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: Data Dictionary \n",
    "In this section, you will be generating a data dictionary for your dataset. Data dictionaries are extremely important since they give us a glance at the various data types we have. \n",
    "\n",
    "Typically each column of your dataset will have a data type associate with it. The data types that you would be typically used are: \n",
    "\n",
    "- Int\n",
    "- Float\n",
    "- String\n",
    "- Categorical\n",
    "- Datetime \n",
    "\n",
    "Try to cast each variable into one of these data types. The data dictionary would be a table with: \n",
    "- One column as a feature/target name\n",
    "- One column for the data type\n",
    "- One column with a short description of the feature/target\n",
    "\n",
    "You can generate markdown tables at :\n",
    "https://www.tablesgenerator.com/markdown_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Extraction\n",
    "\n",
    "- If you have unstructured data then in this step you need to extract features from the data to generate a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data cleaning\n",
    "\n",
    "Some points to keep in mind: \n",
    "\n",
    "- [ ] Find missing values. \n",
    "- [ ] Find NaN and 0 values. \n",
    "- [ ] Do all columns have the same dtypes?\n",
    "- [ ] Convert dates to datetime types.\n",
    "    - [ ] You can use the python package arrow or datetime.\n",
    "- [ ] Convert categorical variables to type 'category' if working with pandas. \n",
    "- [ ] Convert strings to ints or floats if they represent numbers.\n",
    "- [ ] Standardize strings\n",
    "    - [ ] Convert them to lower case if possible.\n",
    "    - [ ] Replace spaces with underscores or dashes.\n",
    "    - [ ] Remove white spaces around the string **this is very critical**.\n",
    "    - [ ] Check of inconsistent spellings *typically done manually*.\n",
    "- [ ] Look for duplicate rows or columns.\n",
    "- [ ] Look for preprocessed columns; example: A categorical column that has been duplicated \n",
    "    with categorical labels.\n",
    "    \n",
    "A list of data cleaning libraries: https://mode.com/blog/python-data-cleaning-libraries/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6:  Data preperation\n",
    "\n",
    "- [ ] Convert categorical features to dummy indices if you are doing regression or assign numerical labels if you are doing classification\n",
    "- [ ] Do test train split to generate a test set. Further do a train validation split, you will need to run the test train split function from sklearn twice for this purpose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Exploratory Data Analysis and Data Visualization\n",
    "\n",
    "There are multiple steps that you need to take here: \n",
    "\n",
    "- [ ] Identify outliers in the datsets. Keep track of them, we want to run to train the model with the outliers and without them to see their effect.\n",
    "- [ ] Check for imbalance in the target variable. Quantify the imbalance. \n",
    "- [ ] Pairplot if possible to check the relationship between all the features and the target.\n",
    "- [ ] Look at the histogram for each variable, try to identify if you have a symmetric or normal distribution.\n",
    "- [ ] If possible plot a QQ plot to check the normality of the data. If you want more information, refer to [this](https://refactored.ai/learn/normality-tests/24c311b1936a4037b29ef78d629f1320/).\n",
    "- [ ] If it's a classification problem, run a chi-square test between each categorical feature and the target to check for correlation and run ANOVE between the continuous/discrete features and the target to check for correlations.\n",
    "- [ ] If it's a regression problem get Pearson correlations between the continuous features and target and run ANOVA between each categorical variable and target.\n",
    "- Check for correlations between individual features; use similar approaches as you did with the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Key Insights from EDA\n",
    "In this section, you will present what you have learned from performing Exploratory Data Analysis. At the end of this section, you should have written down the following:\n",
    "\n",
    "- [ ] A bullet point list of the relationship between features and target and between individual features.\n",
    "- [ ] A written summary of the conclusions of the exploratory data analysis.\n",
    "​\n",
    "The first point involves writing down what type of correlations observed between each feature and target. For regression, you can state the correlation value ( r-squared value) between the feature and the target for the classification; you can state the p-value of the chi-square test with the conclusion of whether you are accepting or reject the null hypothesis. The same should be done for between individual features. \n",
    "​\n",
    "The second part involves writing a small summary of part 1. It would be best if you mentioned in words the conclusion that you reached. \n",
    "\n",
    "There may be situations where you may be compelled to drop a variable because you have either too many outliers or strong correlations with the output. You may even want to create new variables using external data that you have imported. You should document and discuss these changes in this section. \n",
    "\n",
    "If you are working on a problem that has industry relevance, this is also a place where you can discuss the results in the context of you the industry. \n",
    "\n",
    "### Data visualizations\n",
    "\n",
    "For each pair of variables (i.e, feature vs. feature and feature vs. target) \n",
    "you need to have a separate section with visualizations. Based on the type of target and feature, you may need to choose an appropriate plot (Histogram, time-series plots, scatter plots, etc.)\n",
    "\n",
    "Make sure that you label the plots properly: \n",
    "Each plot should have the following: \n",
    "- [ ] Readable and descriptive axis labels.\n",
    "- [ ] Font size of at least 12 for the x-y axis tick labels.\n",
    "- [ ] A title.\n",
    "- [ ] A legend that labels a curve. Even if you have a single curve.\n",
    "- [ ] If you are using a scatter plot, no fancy points, use circles, triangles, and other simple geometric shapes.\n",
    "- [ ] Make sure that everything is easy to read plot. Use colors meaningfully; most plots do not need several colors. \n",
    "- [ ] Try to make sure that you start at the origin (x=0, y=0). Be aware of the scale of your data. \n",
    "\n",
    "Ensure you save your final visualizations in the ```/images/``` folder and call them from the images folder. Make sure you are given the images meaningful names. For example, you can name an EDA image between two features as \"eda_scatter_feature1_feature2.png\". Everyone has their naming convention. Make sure that you stick to that convention. \n",
    " \n",
    "### Importing Images into a Jupyter notebook \n",
    " There may be times where you might want to import images into jupyter notebooks. These might be other plots or plots you have generated and saved. \n",
    " \n",
    " Images can be imported to a jupyter notebook from a different directory using a relative path. \n",
    "A realtive path example is: \n",
    "\n",
    "```../../../images/image1.png```\n",
    "\n",
    "This means that we are going 3 folders up and then into the image folder and referencing ```image1.png```. \n",
    "In order to import the image, we would write.\n",
    "\n",
    "```<img src=\"../../../images/image1.png\">``` \n",
    "\n",
    "Where we are using the ```img``` html tag to generate the image, this is written in a markdown cell of the jupyter notebook. NOT a code cell. \n",
    "\n",
    "It is best practice to write down the above html tag in a separate markdown cell. This makes it easier to access and troubleshoot. \n",
    "\n",
    "**Note: If you are a DS1 student, then head straight to the summary section**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 9: Model building\n",
    "\n",
    "This section is relevant to only DS2 and DS3 students. \n",
    "\n",
    "Here you will train a model on your training set. Make sure you train the model then get predictions on the training set. We want to keep track of this since we want to check for overfitting. We will compare the results from the training set to the testing set to see if we are overfitting. \n",
    "\n",
    "- [ ] You should have a plot that shows the training set points, and training set predictions overlaid\n",
    "\n",
    "Typically, if you have high training accuracy (or low r-square value for regression) and low testing set accuracy ( or high r-squared value for regression), it means you are overfitting. In such a situation, you need to go to a more complex model. Perhaps use regularization or get more data. \n",
    "\n",
    "\n",
    "For DS2 students: So far, you have learned- Linear regression, Polynomial regression, Logistic regression and Naive Bayes. Hence you get to choose from these algorithms based on the type of problem, classification/regression, that you have chosen.\n",
    "\n",
    "For DS3 students: You have learned many algorithms and techniques; hence this a place to try them out. Start with a simple baseline model and then try more complex models. Classification example: \n",
    "- Start with baseline model as - Logistic regression or Naive Bayes\n",
    "- Then try the Decision tree\n",
    "- Then try SVC \n",
    "- Then Random forests\n",
    "- Then XGBoost\n",
    "\n",
    "The same can be said for regression. Running multiple models and looking at the accuracy and confusion matrix will help you understand how to judge each model on the training set. \n",
    "\n",
    "If possible, you want to utilize grid search to find parameters. Grid search is great for finding parameters, especially when you have many of them. \n",
    "https://scikit-learn.org/stable/modules/grid_search.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Model evaluation \n",
    "Once you train your model, you need to evaluate how good your model is with the test set.\n",
    "For this section, you need to do the following: \n",
    "- [ ] Run predictions on the test set. \n",
    "- [ ] Clearly state the difference between the prediction metrics of the test set and training set\n",
    "- [ ] Discuss the difference between test set predictions and training set predictions. Is there overfitting? \n",
    "- [ ] If there is overfitting, you may have to go back to the previous step and try other approaches like cross-validation. \n",
    "- [ ] If you do have overfitting, you should document how you dealt with it. \n",
    "[ ] In the case of classification, you should discuss the confusion matrix's implications if you are dealing only with two classes.\n",
    "- [ ] You should have a plot that shows the test set points, and predictions overlaid.\n",
    "- [ ] Are there specific features that substantially affect the outcome? Look at correlations between the features and the predictions.\n",
    "- [ ] In some cases, you might be able to use feature importances (especially for tree methods) feature/ set of features contribute heavily to the model's success. State and plot those here. You will discuss them in the next section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Key Insights from Predictive analysis \n",
    "Once you finish building and evaluating your model, you should include the following in this section: \n",
    "- [ ] What was the conclusion of the model building and model evaluation steps? \n",
    "- [ ] A discussion on the relevance of the predictions to the business problem. \n",
    "    - [ ] Does the result inform existing/known relationships between features and target\n",
    "    - [ ] Does the result teach us something new about the relationship between the features and the target?\n",
    "    - [ ] Does the predictive model help us better understand the problem and its solution? \n",
    "    - [ ] Discuss feature importances (if you can extract them) and their relevance to the business problem \n",
    "    \n",
    "- [ ] If there are any connections between the EDA section and this section, make sure you highlight them. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 12: Project Summary\n",
    "\n",
    "The project summary is the overview of the entire project.\n",
    "- [ ] Describe the conclusion you arrived at with this project. Discuss it in the context of the Problem Statement. \n",
    "- [ ] A summary of steps you took in the EDA\n",
    "- [ ] A summary of steps you tool modeling the data\n",
    "- [ ] A brief summary of the conclusions from doing EDA and model building\n",
    "- [ ] How can you use this result/model? What is the realm of applicability? \n",
    "- [ ] What general conclusion can you draw from this project and analysis? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}