{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "s1",
     "content",
     "l1"
    ]
   },
   "source": [
    "# Advanced Linear Regression\n",
    "<br/>\n",
    "# Improving the fit - Cross Validation\n",
    "\n",
    "## Cross Validation\n",
    "\n",
    "Until now, we have been training the model with a dataset and measuring the performance on the same dataset. This isnt a good way to measure the performance of a model as in reality the test set is different from the training dataset. Hence, the performance on the test set is the right way to evaluate how good the model is. Since test sets are not always available, evaluating the model on the data it hasn't been trained on will provide a true estimate of the performance. Cross Validation is a technique of splitting the input data set into training and test set with a ratio where majority is the training set. A one way split might not train the model for all the variance. Hence, we go on splitting the data set in different parts so that each time the test set is different from the previous split. The cross-validation splits are shown in the figure below. If we consider k splits, it is called as k-fold cross validation. K-fold cross validation will train k different models and evaluate the performance on k different test sets and consider the mean of all the errors as the benchmark performance error. Below is the visualization of how train-test splits are selected by varying the  window so that the model is evaluated with all different portions of data.\n",
    "\n",
    "<br/><br/>\n",
    "<div style='float: left;'><img src='../../../images/vis_7.png'></div>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "## Cross Validation using sklearn\n",
    "\n",
    "cross validation is available in sklearn. Let us perform a 5-fold cross_validation on the Boston Housing dataset.\n",
    "\n",
    "\n",
    "## Crosss Validation (CV) using Scikit-Learn\n",
    "\n",
    "Import cross_validation and metrics function from sklearn. cross_validation is to perform CV on the dataset and metrics is used to evaluate the performance using the mean_squared_error function that it offers.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, metrics\n",
    "```\n",
    "The target is the Housing Price variable 'MEDV' which we shall assign to the variable y.\n",
    "\n",
    "```python\n",
    "y = boston_data['MEDV']\n",
    "```\n",
    "Instantiate a 5 fold split using the KFold function on the boston_data:\n",
    "\n",
    "```python\n",
    "k_folds = KFold(n_splits=n_fold_var)\n",
    "```\n",
    "\n",
    "What the above command does is that it creates pairs of train and test indices that splits the dataset as explained in the illustration above. Hence we have 5 pairs of such splits of training and testing data over various parts. We can now iterate over all the parts and evaluate the Mean Squared Error (MSE) over each such split. We shall use a scores list to append all the MSEs for each such split. The iloc function over the dataframe can reference the values by index. Therefore, the part of the dataset where testing data or 'unseen' data is provided by KFold function can be accessed.\n",
    "\n",
    "```python\n",
    "scores = []\n",
    "for train_index, test_index in k_folds.split(boston_data):\n",
    "    lm = linear_model.LinearRegression().fit(boston_data.iloc[train_index], y.iloc[train_index])\n",
    "    y_hat = lm.predict(boston_data.iloc[test_index])\n",
    "    scores.append(metrics.mean_squared_error(y.iloc[test_index], y_hat))\n",
    "```\n",
    "Taking the mean of all such MSEs over all splits will give us a value that we can use to 'honestly' evaluate our model:\n",
    "\n",
    "```python\n",
    "print(np.mean(scores))\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "###  Instructions\n",
    " Peform k-fold cross validation where k=3 without randomizing the dataset and determine:\n",
    "\n",
    "- Mean scores and assign it to mean_scores variable. \n",
    "- Set the variables with _var appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "s1",
     "ce",
     "l1"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection, metrics\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import rcParams\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "boston_dataset = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_data['MEDV'] = boston_dataset.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "tags": [
     "s1",
     "l1",
     "ans"
    ]
   },
   "source": [
    "## Solution\n",
    "\n",
    "```python\n",
    "# Perform linear regression using sklearn\n",
    "reg_model = linear_model.LinearRegression()\n",
    "X = boston_data[['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
    "y = boston_data['MEDV']\n",
    "reg_model.fit(X, y)\n",
    "\n",
    "n_fold_var = 3\n",
    "k_folds = KFold(n_splits=n_fold_var)\n",
    "scores = []\n",
    "for train_index, test_index in k_folds.split(X):\n",
    "    lm = linear_model.LinearRegression().fit(boston_data.iloc[train_index], y.iloc[train_index])\n",
    "    y_hat = lm.predict(boston_data.iloc[test_index])\n",
    "    scores.append(metrics.mean_squared_error(y.iloc[test_index], y_hat))\n",
    "\n",
    "mean_scores = np.mean(scores)\n",
    "print(mean_scores)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the above code here \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l2",
     "content",
     "s2"
    ]
   },
   "source": [
    "\n",
    "<br/><br/><br/>\n",
    "# Regression using Scikit-Learn\n",
    "\n",
    "Scikit-Learn can also be used for linear regression. \n",
    "\n",
    "```python\n",
    "from sklearn import linear_model, feature_selection\n",
    "reg_model = linear_model.LinearRegression()\n",
    "X = boston_data[['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
    "y = boston_data['MEDV']\n",
    "reg_model.fit(X, y)\n",
    "p_values = feature_selection.f_regression(X, y)[1]\n",
    "residuals = (y-reg_model.predict(X)).values\n",
    "```\n",
    "The residuals are errors and we can plot a distribution of these errors using distplot:\n",
    "\n",
    "```python\n",
    "g = sns.distplot(residuals, color=\"m\")\n",
    "sns.plot.show()\n",
    "```\n",
    "<br/>\n",
    "<div style='float: left;'><img src='../../../images/figure_6.png'></div>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "The residuals follow closely resembles gaussian distribution which shows that the data is good for modeling using linear regression.\n",
    "\n",
    "<br/>\n",
    "## Visualizing Regression Line Fit \n",
    "The linear regression fits a line to the data set. We can visualize the line when a single variable is involved in the modeling. We can henceforth, imagine a hyper-plane extended in several dimensions. Going back to the modeling of housing rate with crime rate variable, specifying kind argument as \"reg\" in the jointplot, the best line that is fit can be seen.\n",
    "\n",
    "```python\n",
    "sns.jointplot(x=\"CRIM\", y=\"MEDV\", data=boston_data, kind=\"reg\")\n",
    "```\n",
    "<br/>\n",
    "<div style='float: left;'><img src='../../../images/figure_7.png'></div>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "The line in the above figure shows the best fit to the crime rate for predicting housing prices. \n",
    "\n",
    "<br/>\n",
    "\n",
    "### Instructions\n",
    "\n",
    "Model the house prices for percent of lower status of the population using scikit-learn.\n",
    "\n",
    "- Also generate a joint plot for the above data and assign the plot to g.\n",
    "- Determine mean squared error and assign it to mse_lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "l2",
     "ce",
     "s2"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, feature_selection\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Modify the code below to predict housing prices for LSTAT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "tags": [
     "l2",
     "s2",
     "ans"
    ]
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "reg_model = linear_model.LinearRegression()\n",
    "X = boston_data[['LSTAT']]\n",
    "y = boston_data['MEDV']\n",
    "reg_model.fit(X, y)\n",
    "p_values = feature_selection.f_regression(X, y)[1]\n",
    "y_hat = reg_model.predict(X)\n",
    "mse_lm = mean_squared_error(y_hat, y)\n",
    "g = sns.jointplot(x=\"LSTAT\", y=\"MEDV\", data=boston_data, kind=\"reg\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We saw in previous sections that, in Linear Regression to get the best fit line , we need to reduce the MSE value to a minimum possible value. To identify a slope & intercept, we use the equation:\n",
    "\n",
    "$$ y = mx + b $$\n",
    "\n",
    "where:\n",
    "\n",
    "‘m’ is the slope\n",
    "\n",
    "‘x’ → independent variables\n",
    "\n",
    "‘b’ is intercept\n",
    "\n",
    "However this applies only to a Univariate Linear Regression where we have one target variable. In case of multivatriate Linear Regression, we need to apply what is called <b>Gradient Descent</b>.\n",
    "\n",
    "Gradient descent algorithm’s main objective is to minimise the cost function. It is one of the best optimisation algorithms to minimise errors (difference of actual value and predicted value).\n",
    "\n",
    "The goal is similar like the above operation that we did to find out a best fit of intercept line ‘y’ in the slope ‘m’. Using Gradient descent algorithm also, we will figure out a minimal cost function by applying various parameters for theta 0 and theta 1 and see the slope intercept until it reaches convergence.\n",
    "\n",
    "In a real world example, it is similar to find out a best direction to take a step downhill. We take a step towards the direction to get down. From the each step, you look out the direction again to get down faster and downhill quickly. The similar approach is using in this algorithm to minimise cost function.\n",
    "\n",
    "Similarly, starting at the top of the mountain in curve, we take our first step downhill in the direction specified by the negative gradient. Next we recalculate the negative gradient (passing in the coordinates of our new point) and take another step in the direction it specifies. We continue this process iteratively until we get to the bottom of our graph, or to a point where we can no longer move downhill–a local minimum.\n",
    "\n",
    "<br/>\n",
    "<div style='float: left;'><img src='../../../images/gradient_descent.png' style=\"width:55vw\"></div>\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "The size of these steps is called the learning rate. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "There are two parameters in our cost function we can control: m (weight) and b (bias). Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.\n",
    "\n",
    "The cost function is given by:\n",
    "\n",
    "$$ f(m,b) =  \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2 $$\n",
    "\n",
    "The gradient Descent is given by :\n",
    "\n",
    "$\\begin{split}f'(m,b) =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{df}{dm}\\\\\n",
    "     \\frac{df}{db}\\\\\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{1}{N} \\sum -2x_i(y_i - (mx_i + b)) \\\\\n",
    "     \\frac{1}{N} \\sum -2(y_i - (mx_i + b)) \\\\\n",
    "    \\end{bmatrix}\\end{split}$\n",
    "    \n",
    "\n",
    "Types of Gradient Descent:\n",
    "\n",
    "Gradient Descent has three variants:\n",
    "\n",
    "<li>Batch gradient descent\n",
    "<li>Stochastic gradient descent\n",
    "<li>Mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Gradient Descent code\n",
    "# Hit run\n",
    "next_x = 6  # We start the search at x=6\n",
    "gamma = 0.01  # Step size multiplier\n",
    "precision = 0.00001  # Desired precision of result\n",
    "max_iters = 10000  # Maximum number of iterations\n",
    "\n",
    "# Derivative function\n",
    "df = lambda x: 4 * x**3 - 9 * x**2\n",
    "\n",
    "for i in range(max_iters):\n",
    "    current_x = next_x\n",
    "    next_x = current_x - gamma * df(current_x)\n",
    "    step = next_x - current_x\n",
    "    if abs(step) <= precision:\n",
    "        break\n",
    "\n",
    "print(\"Minimum at\", next_x)\n",
    "\n",
    "# The output for the above will be something like\n",
    "# \"Minimum at 2.2499646074278457\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Solution\n",
    "\n",
    "Just run the above code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "executed_sections": [],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
