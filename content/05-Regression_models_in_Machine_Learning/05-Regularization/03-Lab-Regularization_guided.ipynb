{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-Lab  - Regularization- Ridge, Lasso and ElasticNet\n",
    "\n",
    "In this lab, you will learn how to implement Regularization techniques using sklearn and Statsmodels.  You will work with Credit dataset - description provided below .\n",
    "The dataset contains information about credit information of few people and their other details. The goal is to predict the Bank Balance of each individual based on their credit information. \n",
    "Your task is divided into three parts: \n",
    "1) Part 1 : Read the dataset, convert categorical dataset to dummy indicies so we can use them for regression and Visual the dataset.<br>\n",
    "2) Perform a simple polynomaial regression using Sklearn and compare it with Ridge, Lasso and ElasticNet regression.  <br>\n",
    "3) Use Statsmodels OLS regression to perform linear regression and regression using Elastic Net <br>\n",
    "\n",
    "Data is availabe in : https://raw.githubusercontent.com/colaberry/DSin100days/master/data/Credit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Write your code here or in other code cells down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning and Prep\n",
    "In this part you will be importing the data. Reading the data into a dataframe. You will need to use pandas to create dummy indicies covert categorical variable dummy columns. You will then generate the test train splits for the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 400 entries, 0 to 399\n",
      "Data columns (total 12 columns):\n",
      "Unnamed: 0    400 non-null int64\n",
      "Income        400 non-null float64\n",
      "Limit         400 non-null int64\n",
      "Rating        400 non-null int64\n",
      "Cards         400 non-null int64\n",
      "Age           400 non-null int64\n",
      "Education     400 non-null int64\n",
      "Gender        400 non-null object\n",
      "Student       400 non-null object\n",
      "Married       400 non-null object\n",
      "Ethnicity     400 non-null object\n",
      "Balance       400 non-null int64\n",
      "dtypes: float64(1), int64(7), object(4)\n",
      "memory usage: 40.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Balance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.891</td>\n",
       "      <td>3606</td>\n",
       "      <td>283</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>106.025</td>\n",
       "      <td>6645</td>\n",
       "      <td>483</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>Female</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Asian</td>\n",
       "      <td>903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>104.593</td>\n",
       "      <td>7075</td>\n",
       "      <td>514</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>11</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>148.924</td>\n",
       "      <td>9504</td>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Asian</td>\n",
       "      <td>964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>55.882</td>\n",
       "      <td>4897</td>\n",
       "      <td>357</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Income  Limit  Rating  Cards  Age  Education  Gender Student  \\\n",
       "0           1   14.891   3606     283      2   34         11    Male      No   \n",
       "1           2  106.025   6645     483      3   82         15  Female     Yes   \n",
       "2           3  104.593   7075     514      4   71         11    Male      No   \n",
       "3           4  148.924   9504     681      3   36         11  Female      No   \n",
       "4           5   55.882   4897     357      2   68         16    Male      No   \n",
       "\n",
       "  Married  Ethnicity  Balance  \n",
       "0     Yes  Caucasian      333  \n",
       "1     Yes      Asian      903  \n",
       "2      No      Asian      580  \n",
       "3      No      Asian      964  \n",
       "4     Yes  Caucasian      331  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the dataset. We have provided the file location in file_loc. Use pandas to read the csv file\n",
    "# make sure you include na_values as '?'. Make sure you also do dropna to remove nan values. \n",
    "# You will have to show by df.info() and df.head() (in that order) to get the output shown.\n",
    "file_loc = 'https://raw.githubusercontent.com/colaberry/DSin100days/master/data/Credit.csv'\n",
    "credit = \"####\"\n",
    "credit\"####\"\n",
    "credit\"####\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/regularization_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of each columns is: \n",
      "[dtype('int64') dtype('float64') dtype('int64') dtype('int64')\n",
      " dtype('int64') dtype('int64') dtype('int64') dtype('int64')\n",
      " dtype('uint8') dtype('uint8') dtype('uint8') dtype('uint8')\n",
      " dtype('uint8') dtype('uint8') dtype('uint8') dtype('uint8')\n",
      " dtype('uint8')]\n"
     ]
    }
   ],
   "source": [
    "# You will have to convert categorical variables to dummy indicies. Pandas lets us do this very easily \n",
    "# with a function called get_dummies. Use that to get dummy indicies. Get the dtypes of the df with \n",
    "# dummy variables and get the values of those dtypes. This what we want you to print as a result.\n",
    "credit_with_dummies = \"####\"\n",
    "check_all_variables =\"####\"\n",
    "print(\"Data type of each columns is: \\n{}\".format(check_all_variables))\n",
    "\n",
    "# You need to drop unnamed column from the dataframe. Look up the columns and ensure it this column is dropped\n",
    "# Note: If you do not drop the column then your results will not match from this point on.\n",
    "credit_with_dummies = \"####\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type of each columns is: <br>\n",
    "[dtype('int64') dtype('float64') dtype('int64') dtype('int64') <br>\n",
    " dtype('int64') dtype('int64') dtype('int64') dtype('int64') <br>\n",
    " dtype('uint8') dtype('uint8') dtype('uint8') dtype('uint8') <br>\n",
    " dtype('uint8') dtype('uint8') dtype('uint8') dtype('uint8') <br>\n",
    " dtype('uint8')] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if any values are null we get: \n",
      "[False False False False False False False False False False False False\n",
      " False False False False]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# checking for missing values \n",
    "\n",
    "# We need to check if nay of the columns have any missing values. You can do that by using isnull() and any() \n",
    "# functions. Both are methods that can be called on a df. For example you can do  df.isnull(). \n",
    "check_missing = \"####\"\n",
    "print(\"Check if any values are null we get: \\n{}\".format(check_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any values are null we get: <br>\n",
    "[False False False False False False False False False False False False  <br>\n",
    " False False False False False] <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the data our first step is prepare the data for Linear regression. For this we need to split our data into the features and target. Below you will do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need to drop the traget column. In the question description we \n",
    "# have mentioned the variable you will be regressing for, so drop that column \n",
    "# to get all the features in the dataset.\n",
    "X =\"####\"\n",
    "\n",
    "# The column we dropped above will be the target hence y must equal that column.\n",
    "y =\"####\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn's pairplot to show how each of the features vary with each other\n",
    "# note this operation may take upto a minute since we have a lot of columns\n",
    "# Make sure you pairplot X and not y since y will only contain 1 column \n",
    "fig = \"####\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/regularization_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pair plot we can see that we have a strong correlation between ```ratings``` and ```Limit``` variables, hence we will drop one of these. We will next drop the ```Limit``` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Limit variable from X\n",
    "X =\"####\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Gender_ Male</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Student_No</th>\n",
       "      <th>Student_Yes</th>\n",
       "      <th>Married_No</th>\n",
       "      <th>Married_Yes</th>\n",
       "      <th>Ethnicity_African American</th>\n",
       "      <th>Ethnicity_Asian</th>\n",
       "      <th>Ethnicity_Caucasian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.861583</td>\n",
       "      <td>-0.465539</td>\n",
       "      <td>-0.699130</td>\n",
       "      <td>-1.257674</td>\n",
       "      <td>-0.784930</td>\n",
       "      <td>1.035635</td>\n",
       "      <td>-1.035635</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.795395</td>\n",
       "      <td>0.795395</td>\n",
       "      <td>-0.573501</td>\n",
       "      <td>-0.585049</td>\n",
       "      <td>1.005013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.727437</td>\n",
       "      <td>0.828703</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>1.528451</td>\n",
       "      <td>0.496588</td>\n",
       "      <td>-0.965592</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.795395</td>\n",
       "      <td>0.795395</td>\n",
       "      <td>-0.573501</td>\n",
       "      <td>1.709260</td>\n",
       "      <td>-0.995012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.686756</td>\n",
       "      <td>1.029311</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.889964</td>\n",
       "      <td>-0.784930</td>\n",
       "      <td>1.035635</td>\n",
       "      <td>-1.035635</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.257237</td>\n",
       "      <td>-1.257237</td>\n",
       "      <td>-0.573501</td>\n",
       "      <td>1.709260</td>\n",
       "      <td>-0.995012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.946152</td>\n",
       "      <td>2.110003</td>\n",
       "      <td>0.031032</td>\n",
       "      <td>-1.141586</td>\n",
       "      <td>-0.784930</td>\n",
       "      <td>-0.965592</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.257237</td>\n",
       "      <td>-1.257237</td>\n",
       "      <td>-0.573501</td>\n",
       "      <td>1.709260</td>\n",
       "      <td>-0.995012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302928</td>\n",
       "      <td>0.013331</td>\n",
       "      <td>-0.699130</td>\n",
       "      <td>0.715831</td>\n",
       "      <td>0.816968</td>\n",
       "      <td>1.035635</td>\n",
       "      <td>-1.035635</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.795395</td>\n",
       "      <td>0.795395</td>\n",
       "      <td>-0.573501</td>\n",
       "      <td>-0.585049</td>\n",
       "      <td>1.005013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Income    Rating     Cards       Age  Education  Gender_ Male  \\\n",
       "0 -0.861583 -0.465539 -0.699130 -1.257674  -0.784930      1.035635   \n",
       "1  1.727437  0.828703  0.031032  1.528451   0.496588     -0.965592   \n",
       "2  1.686756  1.029311  0.761194  0.889964  -0.784930      1.035635   \n",
       "3  2.946152  2.110003  0.031032 -1.141586  -0.784930     -0.965592   \n",
       "4  0.302928  0.013331 -0.699130  0.715831   0.816968      1.035635   \n",
       "\n",
       "   Gender_Female  Student_No  Student_Yes  Married_No  Married_Yes  \\\n",
       "0      -1.035635    0.333333    -0.333333   -0.795395     0.795395   \n",
       "1       0.965592   -3.000000     3.000000   -0.795395     0.795395   \n",
       "2      -1.035635    0.333333    -0.333333    1.257237    -1.257237   \n",
       "3       0.965592    0.333333    -0.333333    1.257237    -1.257237   \n",
       "4      -1.035635    0.333333    -0.333333   -0.795395     0.795395   \n",
       "\n",
       "   Ethnicity_African American  Ethnicity_Asian  Ethnicity_Caucasian  \n",
       "0                   -0.573501        -0.585049             1.005013  \n",
       "1                   -0.573501         1.709260            -0.995012  \n",
       "2                   -0.573501         1.709260            -0.995012  \n",
       "3                   -0.573501         1.709260            -0.995012  \n",
       "4                   -0.573501        -0.585049             1.005013  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next use standard scalar to scale and center the dataset.  \n",
    "scaler = \"####\"\n",
    "\n",
    "# Create a new dataset where the data is the fit tranformed output of X and \n",
    "# columns are the X.columns.\n",
    "scaled_X =\"####\"\n",
    "scaled_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../images/regularization_3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of transformed training set: (280, 120)\n",
      "Shape of transformed testing set: (120, 120)\n"
     ]
    }
   ],
   "source": [
    "# Use train test split to split the dataset into training set and testing set. \n",
    "# use a test size of 0.3 and random_state of 2\n",
    "X_train, X_test, y_train, y_test= \"####\"\n",
    "\n",
    "# Next declare an instance of Polynomial features class from sklearn with degree 2. We will\n",
    "# use this to create polynomial features\n",
    "pf = \"####\"\n",
    "\n",
    "# you will need to fit transform the training data features and the testing set features to generate polynomial feature.\n",
    "poly_train_X = \"####\"\n",
    "poly_test_X = \"####\"\n",
    "\n",
    "\n",
    "# Add a new axis to y_train and y_test so you can perform linear regression \n",
    "y_train = \"####\"\n",
    "y_test = \"####\"\n",
    "\n",
    "\n",
    "# Shape of the polynomial traning and testing sets\n",
    "print(\"Shape of transformed training set: {}\".format(poly_train_X.shape))\n",
    "print(\"Shape of transformed testing set: {}\".format(poly_test_X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of transformed training set: (280, 136) <br>\n",
    "Shape of transformed testing set: (120, 136)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear, Ridge, Lasso and ElasticNet regression using Sklearn \n",
    "\n",
    "Next we are going to use the transformed data to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 7149.13\n"
     ]
    }
   ],
   "source": [
    "# Delcare an instance of Linear Regression class from Sklearn, make sure you set fit_interpet to False\n",
    "# We set the intercept to false since we have already centered the data using standard scalar.\n",
    "lr = \"####\"\n",
    "\n",
    "# Fit the training data with polynomial features to the linear regression model. \n",
    "lr.\"####\"\n",
    "\n",
    "# Predict on the test set. Remember the test set must also be transformed to polynomial features. \n",
    "y_pred = \"####\"\n",
    "\n",
    "# Calculate the mean squared error using the function from Sklearn \n",
    "mse = \"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 7149.13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to try Ridge and Lasso regression from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 7381.96\n"
     ]
    }
   ],
   "source": [
    "# Delcare an instance of Ridge class, set alpha =1.2 normalize=False and max_iter=10000.\n",
    "lr_ridge = \"####\"\n",
    "\n",
    "# similar to linear regression fit the polynomial features training set. \n",
    "\"####\"\n",
    "\n",
    "# Run pedict on the test set.\n",
    "\"####\"\n",
    "# Calculate the mean squared error using the function from Sklearn.\n",
    "mse = \"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 7381.96\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 6673.42\n"
     ]
    }
   ],
   "source": [
    "# Delcare an instance of Lasso class, set alpha =1.1 normalize=False and max_iter=10000.\n",
    "lr_lasso = \"####\"\n",
    "\n",
    "# similar to linear regression fit the polynomial features training set. \n",
    "lr_lasso.\"####\"\n",
    "\n",
    "# Run pedict on the test set.\n",
    "y_pred = \"####\"\n",
    "\n",
    "# Calculate the mean squared error using the function from Sklearn. \n",
    "mse = \"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 6673.42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 89630.78\n"
     ]
    }
   ],
   "source": [
    "# Delcare an instance of  ElasticNet , set alpha =1.5, l1_ratio=0.5,  fit_intercept=False and random_state=0.\n",
    "lr_en = \"####\"\n",
    "lr_en.fit(poly_train_X, y_train)\n",
    "\n",
    "# Predictions work the same way as sklearn. Run predict on the test set. \n",
    "pred_y = lr_en.predict(poly_test_X)\n",
    "\n",
    "# Calculate the mean squared error using the function from Sklearn. \n",
    "mse =\"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 89630.78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that we performed better with Lasso regression than with Ridge. This is because Lasso typically imposes heavy penalty on points which deviate from away from the mean. \n",
    "Next part we look at how we can run linear regression and elastic net in statsmodels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LR and elastic net using Statsmodels\n",
    "Next we are going to use statsmodels OLS regression perform linear regression. You may want to check the documentation for statsmodels [here](https://www.statsmodels.org/stable/regression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 7143.16\n"
     ]
    }
   ],
   "source": [
    "# Using sm.OLS fit a regression model. The sytax for statsmodels is a bit different\n",
    "# you will need chain fit() to sm.OLS(y,X). Remember you X must be composed of polynomial features.\n",
    "st_model = \"####\"\n",
    "\n",
    "# Predictions work the same way as sklearn. Run predict on the test set. \n",
    "pred_y = \"####\"\n",
    "\n",
    "# Calculate the mean squared error using the function from Sklearn.\n",
    "mse = \"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 7143.16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will train a statsmodels model using elastic net. To do this will be using fit_regualized option from statsmodel. This option is a method of the OLS model that we train for linear regression i.e we just replace the fit() method in OLS with fit regularized. You can read the documentation [here](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html) to better understand what each of the arguments do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 90526.54\n"
     ]
    }
   ],
   "source": [
    "# Fit a elastic net model to the training data. fit_regularized is a method of sm.OLS(y,X). Remember to set\n",
    "# the following arguments for fit regularized: method='elastic_net', alpha=1.5 and L1_wt as 0.5 \n",
    "rmodel = \"####\"\n",
    "\n",
    "# Predictions work the same way as sklearn. Run predict on the test set \n",
    "pred_y = \"####\"\n",
    "\n",
    "# Calculate the mean squared error using the function from Sklearn \n",
    "mse = \"####\"\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean squared error: 90526.54\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find that the ElasticNet results for Sklearn and Statsmodels will be pretty much be the same. The same will apply for Linear Regression, there will be some differences between the results due to the way the calucations are done in the backend by results should not be off by an order of magnitude.\n",
    "\n",
    "You might be wondering why did we even both with Statsmodels? Well run the command in the cell below and you should see various statistical measures and values for the linear regression model. One of the most important quantities you will get from this is the p-value of the fit(```Prob (F-statistic):```) This p-value is derived from an F-test which tells you how significant your fit is. If the fit is less than 0.05 you can state that you fit is significant i.e the test states that changes in your traget variable depend on the changes in your feature, if the p-value is greater than 0.05 then we conclude that the test cannot determine if the target varaible changes due to changes in the features. \n",
    "\n",
    "You will also notice that we have a lot of features, this is because we are using the polynomial features and not the dataset features. The summary will also give us access to the 25% and 95% confidence intervals each of the coefficients.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.981</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.976</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   190.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>2.48e-160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:08:27</td>     <th>  Log-Likelihood:    </th> <td> -1560.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   280</td>      <th>  AIC:               </th> <td>   3242.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   220</td>      <th>  BIC:               </th> <td>   3460.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    59</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   41.7204</td> <td>    1.264</td> <td>   32.995</td> <td> 0.000</td> <td>   39.228</td> <td>   44.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> -282.6646</td> <td>    9.922</td> <td>  -28.489</td> <td> 0.000</td> <td> -302.218</td> <td> -263.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  618.3454</td> <td>    7.755</td> <td>   79.736</td> <td> 0.000</td> <td>  603.062</td> <td>  633.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -2.0472</td> <td>    5.751</td> <td>   -0.356</td> <td> 0.722</td> <td>  -13.381</td> <td>    9.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>  -16.8637</td> <td>    4.903</td> <td>   -3.440</td> <td> 0.001</td> <td>  -26.526</td> <td>   -7.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -3.9881</td> <td>    5.190</td> <td>   -0.768</td> <td> 0.443</td> <td>  -14.216</td> <td>    6.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -3.2633</td> <td>    2.427</td> <td>   -1.345</td> <td> 0.180</td> <td>   -8.046</td> <td>    1.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    3.2633</td> <td>    2.427</td> <td>    1.345</td> <td> 0.180</td> <td>   -1.519</td> <td>    8.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    8.7963</td> <td>    0.535</td> <td>   16.437</td> <td> 0.000</td> <td>    7.742</td> <td>    9.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -8.7963</td> <td>    0.535</td> <td>  -16.437</td> <td> 0.000</td> <td>   -9.851</td> <td>   -7.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>  -17.4252</td> <td>    2.143</td> <td>   -8.130</td> <td> 0.000</td> <td>  -21.649</td> <td>  -13.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   17.4252</td> <td>    2.143</td> <td>    8.130</td> <td> 0.000</td> <td>   13.201</td> <td>   21.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>  -13.3873</td> <td>    1.381</td> <td>   -9.692</td> <td> 0.000</td> <td>  -16.110</td> <td>  -10.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>  -11.1752</td> <td>    1.430</td> <td>   -7.817</td> <td> 0.000</td> <td>  -13.992</td> <td>   -8.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   21.2968</td> <td>    1.848</td> <td>   11.526</td> <td> 0.000</td> <td>   17.655</td> <td>   24.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   52.0410</td> <td>   12.418</td> <td>    4.191</td> <td> 0.000</td> <td>   27.568</td> <td>   76.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> -178.1063</td> <td>   23.913</td> <td>   -7.448</td> <td> 0.000</td> <td> -225.234</td> <td> -130.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    4.8286</td> <td>    9.372</td> <td>    0.515</td> <td> 0.607</td> <td>  -13.641</td> <td>   23.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    6.8128</td> <td>    8.843</td> <td>    0.770</td> <td> 0.442</td> <td>  -10.616</td> <td>   24.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>   -5.8480</td> <td>    9.133</td> <td>   -0.640</td> <td> 0.523</td> <td>  -23.847</td> <td>   12.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.2192</td> <td>    4.205</td> <td>   -0.052</td> <td> 0.958</td> <td>   -8.507</td> <td>    8.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.2192</td> <td>    4.205</td> <td>    0.052</td> <td> 0.958</td> <td>   -8.068</td> <td>    8.507</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   20.9759</td> <td>    4.507</td> <td>    4.654</td> <td> 0.000</td> <td>   12.093</td> <td>   29.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>  -20.9759</td> <td>    4.507</td> <td>   -4.654</td> <td> 0.000</td> <td>  -29.858</td> <td>  -12.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    1.4720</td> <td>    4.478</td> <td>    0.329</td> <td> 0.743</td> <td>   -7.352</td> <td>   10.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -1.4720</td> <td>    4.478</td> <td>   -0.329</td> <td> 0.743</td> <td>  -10.296</td> <td>    7.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -5.7664</td> <td>    6.071</td> <td>   -0.950</td> <td> 0.343</td> <td>  -17.731</td> <td>    6.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   10.7206</td> <td>    6.739</td> <td>    1.591</td> <td> 0.113</td> <td>   -2.561</td> <td>   24.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -4.3684</td> <td>    5.131</td> <td>   -0.851</td> <td> 0.396</td> <td>  -14.481</td> <td>    5.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>  157.9286</td> <td>   13.613</td> <td>   11.601</td> <td> 0.000</td> <td>  131.100</td> <td>  184.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>  -15.1313</td> <td>    8.982</td> <td>   -1.685</td> <td> 0.093</td> <td>  -32.833</td> <td>    2.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>  -12.6743</td> <td>    7.840</td> <td>   -1.617</td> <td> 0.107</td> <td>  -28.125</td> <td>    2.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    9.6102</td> <td>    8.140</td> <td>    1.181</td> <td> 0.239</td> <td>   -6.432</td> <td>   25.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.0451</td> <td>    3.913</td> <td>    0.012</td> <td> 0.991</td> <td>   -7.666</td> <td>    7.756</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>   -0.0451</td> <td>    3.913</td> <td>   -0.012</td> <td> 0.991</td> <td>   -7.756</td> <td>    7.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>  -28.9701</td> <td>    4.245</td> <td>   -6.824</td> <td> 0.000</td> <td>  -37.336</td> <td>  -20.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>   28.9701</td> <td>    4.245</td> <td>    6.824</td> <td> 0.000</td> <td>   20.604</td> <td>   37.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>   -0.2474</td> <td>    4.016</td> <td>   -0.062</td> <td> 0.951</td> <td>   -8.163</td> <td>    7.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.2474</td> <td>    4.016</td> <td>    0.062</td> <td> 0.951</td> <td>   -7.668</td> <td>    8.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   11.6327</td> <td>    5.921</td> <td>    1.965</td> <td> 0.051</td> <td>   -0.037</td> <td>   23.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>   -8.6371</td> <td>    5.480</td> <td>   -1.576</td> <td> 0.116</td> <td>  -19.438</td> <td>    2.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>   -2.5112</td> <td>    4.608</td> <td>   -0.545</td> <td> 0.586</td> <td>  -11.592</td> <td>    6.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    6.7745</td> <td>    3.299</td> <td>    2.053</td> <td> 0.041</td> <td>    0.272</td> <td>   13.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>   -1.6839</td> <td>    5.534</td> <td>   -0.304</td> <td> 0.761</td> <td>  -12.591</td> <td>    9.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    1.6486</td> <td>    4.965</td> <td>    0.332</td> <td> 0.740</td> <td>   -8.136</td> <td>   11.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -4.2154</td> <td>    2.495</td> <td>   -1.689</td> <td> 0.093</td> <td>   -9.133</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>    4.2154</td> <td>    2.495</td> <td>    1.689</td> <td> 0.093</td> <td>   -0.702</td> <td>    9.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -2.3328</td> <td>    2.867</td> <td>   -0.814</td> <td> 0.417</td> <td>   -7.984</td> <td>    3.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    2.3328</td> <td>    2.867</td> <td>    0.814</td> <td> 0.417</td> <td>   -3.318</td> <td>    7.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    3.7512</td> <td>    2.869</td> <td>    1.308</td> <td> 0.192</td> <td>   -1.902</td> <td>    9.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>   -3.7512</td> <td>    2.869</td> <td>   -1.308</td> <td> 0.192</td> <td>   -9.405</td> <td>    1.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>    1.0733</td> <td>    3.844</td> <td>    0.279</td> <td> 0.780</td> <td>   -6.502</td> <td>    8.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>   -3.7196</td> <td>    3.435</td> <td>   -1.083</td> <td> 0.280</td> <td>  -10.489</td> <td>    3.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    2.3162</td> <td>    2.940</td> <td>    0.788</td> <td> 0.432</td> <td>   -3.479</td> <td>    8.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>    1.8425</td> <td>    5.799</td> <td>    0.318</td> <td> 0.751</td> <td>   -9.587</td> <td>   13.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -8.7533</td> <td>    5.024</td> <td>   -1.742</td> <td> 0.083</td> <td>  -18.654</td> <td>    1.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>   -2.8018</td> <td>    2.487</td> <td>   -1.126</td> <td> 0.261</td> <td>   -7.704</td> <td>    2.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>    2.8018</td> <td>    2.487</td> <td>    1.126</td> <td> 0.261</td> <td>   -2.100</td> <td>    7.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td>   -4.7847</td> <td>    3.082</td> <td>   -1.553</td> <td> 0.122</td> <td>  -10.858</td> <td>    1.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>    4.7847</td> <td>    3.082</td> <td>    1.553</td> <td> 0.122</td> <td>   -1.289</td> <td>   10.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>   -1.7467</td> <td>    2.541</td> <td>   -0.687</td> <td> 0.493</td> <td>   -6.755</td> <td>    3.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    1.7467</td> <td>    2.541</td> <td>    0.687</td> <td> 0.493</td> <td>   -3.262</td> <td>    6.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>   -2.0501</td> <td>    3.272</td> <td>   -0.627</td> <td> 0.532</td> <td>   -8.499</td> <td>    4.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>    1.5661</td> <td>    3.539</td> <td>    0.443</td> <td> 0.659</td> <td>   -5.409</td> <td>    8.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>    0.4043</td> <td>    2.895</td> <td>    0.140</td> <td> 0.889</td> <td>   -5.300</td> <td>    6.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>   -2.0565</td> <td>    4.435</td> <td>   -0.464</td> <td> 0.643</td> <td>  -10.798</td> <td>    6.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>   -1.4020</td> <td>    2.580</td> <td>   -0.543</td> <td> 0.587</td> <td>   -6.487</td> <td>    3.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>    1.4020</td> <td>    2.580</td> <td>    0.543</td> <td> 0.587</td> <td>   -3.683</td> <td>    6.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>   -4.3904</td> <td>    3.372</td> <td>   -1.302</td> <td> 0.194</td> <td>  -11.036</td> <td>    2.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>    4.3904</td> <td>    3.372</td> <td>    1.302</td> <td> 0.194</td> <td>   -2.255</td> <td>   11.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>    2.1502</td> <td>    2.498</td> <td>    0.861</td> <td> 0.390</td> <td>   -2.772</td> <td>    7.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>   -2.1502</td> <td>    2.498</td> <td>   -0.861</td> <td> 0.390</td> <td>   -7.073</td> <td>    2.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>    5.0481</td> <td>    3.453</td> <td>    1.462</td> <td> 0.145</td> <td>   -1.757</td> <td>   11.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>   -4.3256</td> <td>    3.548</td> <td>   -1.219</td> <td> 0.224</td> <td>  -11.318</td> <td>    2.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>   -0.5864</td> <td>    2.989</td> <td>   -0.196</td> <td> 0.845</td> <td>   -6.477</td> <td>    5.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>   41.4918</td> <td>    1.256</td> <td>   33.043</td> <td> 0.000</td> <td>   39.017</td> <td>   43.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>  -41.4918</td> <td>    1.256</td> <td>  -33.043</td> <td> 0.000</td> <td>  -43.967</td> <td>  -39.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>   -2.0976</td> <td>    1.318</td> <td>   -1.592</td> <td> 0.113</td> <td>   -4.695</td> <td>    0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>    2.0976</td> <td>    1.318</td> <td>    1.592</td> <td> 0.113</td> <td>   -0.500</td> <td>    4.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>    0.8524</td> <td>    1.224</td> <td>    0.696</td> <td> 0.487</td> <td>   -1.560</td> <td>    3.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>   -0.8524</td> <td>    1.224</td> <td>   -0.696</td> <td> 0.487</td> <td>   -3.265</td> <td>    1.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>   -0.6453</td> <td>    1.685</td> <td>   -0.383</td> <td> 0.702</td> <td>   -3.966</td> <td>    2.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>    1.5803</td> <td>    1.672</td> <td>    0.945</td> <td> 0.346</td> <td>   -1.715</td> <td>    4.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>   -0.8206</td> <td>    1.441</td> <td>   -0.569</td> <td> 0.570</td> <td>   -3.661</td> <td>    2.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>   41.4918</td> <td>    1.256</td> <td>   33.043</td> <td> 0.000</td> <td>   39.017</td> <td>   43.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>    2.0976</td> <td>    1.318</td> <td>    1.592</td> <td> 0.113</td> <td>   -0.500</td> <td>    4.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>   -2.0976</td> <td>    1.318</td> <td>   -1.592</td> <td> 0.113</td> <td>   -4.695</td> <td>    0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>   -0.8524</td> <td>    1.224</td> <td>   -0.696</td> <td> 0.487</td> <td>   -3.265</td> <td>    1.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>    0.8524</td> <td>    1.224</td> <td>    0.696</td> <td> 0.487</td> <td>   -1.560</td> <td>    3.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>    0.6453</td> <td>    1.685</td> <td>    0.383</td> <td> 0.702</td> <td>   -2.675</td> <td>    3.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>   -1.5803</td> <td>    1.672</td> <td>   -0.945</td> <td> 0.346</td> <td>   -4.876</td> <td>    1.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>    0.8206</td> <td>    1.441</td> <td>    0.569</td> <td> 0.570</td> <td>   -2.020</td> <td>    3.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td>   18.2636</td> <td>    0.629</td> <td>   29.027</td> <td> 0.000</td> <td>   17.024</td> <td>   19.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>  -18.2636</td> <td>    0.629</td> <td>  -29.027</td> <td> 0.000</td> <td>  -19.504</td> <td>  -17.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>   -0.2968</td> <td>    1.237</td> <td>   -0.240</td> <td> 0.811</td> <td>   -2.735</td> <td>    2.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>    0.2968</td> <td>    1.237</td> <td>    0.240</td> <td> 0.811</td> <td>   -2.141</td> <td>    2.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>   -1.2728</td> <td>    1.771</td> <td>   -0.719</td> <td> 0.473</td> <td>   -4.763</td> <td>    2.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>    2.2096</td> <td>    1.670</td> <td>    1.323</td> <td> 0.187</td> <td>   -1.082</td> <td>    5.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>   -0.8276</td> <td>    1.646</td> <td>   -0.503</td> <td> 0.616</td> <td>   -4.071</td> <td>    2.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>   18.2636</td> <td>    0.629</td> <td>   29.027</td> <td> 0.000</td> <td>   17.024</td> <td>   19.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>    0.2968</td> <td>    1.237</td> <td>    0.240</td> <td> 0.811</td> <td>   -2.141</td> <td>    2.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>   -0.2968</td> <td>    1.237</td> <td>   -0.240</td> <td> 0.811</td> <td>   -2.735</td> <td>    2.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>    1.2728</td> <td>    1.771</td> <td>    0.719</td> <td> 0.473</td> <td>   -2.217</td> <td>    4.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>   -2.2096</td> <td>    1.670</td> <td>   -1.323</td> <td> 0.187</td> <td>   -5.501</td> <td>    1.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td>    0.8276</td> <td>    1.646</td> <td>    0.503</td> <td> 0.616</td> <td>   -2.416</td> <td>    4.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>   33.6726</td> <td>    1.121</td> <td>   30.050</td> <td> 0.000</td> <td>   31.464</td> <td>   35.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>  -33.6726</td> <td>    1.121</td> <td>  -30.050</td> <td> 0.000</td> <td>  -35.881</td> <td>  -31.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>   -1.0348</td> <td>    1.700</td> <td>   -0.609</td> <td> 0.543</td> <td>   -4.385</td> <td>    2.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td>   -1.9918</td> <td>    1.870</td> <td>   -1.065</td> <td> 0.288</td> <td>   -5.678</td> <td>    1.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td>    2.6296</td> <td>    1.497</td> <td>    1.756</td> <td> 0.080</td> <td>   -0.321</td> <td>    5.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td>   33.6726</td> <td>    1.121</td> <td>   30.050</td> <td> 0.000</td> <td>   31.464</td> <td>   35.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>    1.0348</td> <td>    1.700</td> <td>    0.609</td> <td> 0.543</td> <td>   -2.316</td> <td>    4.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>    1.9918</td> <td>    1.870</td> <td>    1.065</td> <td> 0.288</td> <td>   -1.694</td> <td>    5.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td>   -2.6296</td> <td>    1.497</td> <td>   -1.756</td> <td> 0.080</td> <td>   -5.580</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td>   26.0549</td> <td>    1.610</td> <td>   16.181</td> <td> 0.000</td> <td>   22.881</td> <td>   29.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td>    0.2429</td> <td>    1.063</td> <td>    0.228</td> <td> 0.820</td> <td>   -1.853</td> <td>    2.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td>  -22.7005</td> <td>    1.188</td> <td>  -19.113</td> <td> 0.000</td> <td>  -25.041</td> <td>  -20.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td>   29.1571</td> <td>    1.561</td> <td>   18.682</td> <td> 0.000</td> <td>   26.081</td> <td>   32.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>  -25.6269</td> <td>    1.155</td> <td>  -22.187</td> <td> 0.000</td> <td>  -27.903</td> <td>  -23.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td>   41.9333</td> <td>    1.275</td> <td>   32.899</td> <td> 0.000</td> <td>   39.421</td> <td>   44.445</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.116</td> <th>  Durbin-Watson:     </th> <td>   1.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.944</td> <th>  Jarque-Bera (JB):  </th> <td>   0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.047</td> <th>  Prob(JB):          </th> <td>   0.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.922</td> <th>  Cond. No.          </th> <td>2.94e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.07e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.981\n",
       "Model:                            OLS   Adj. R-squared:                  0.976\n",
       "Method:                 Least Squares   F-statistic:                     190.6\n",
       "Date:                Tue, 05 Jan 2021   Prob (F-statistic):          2.48e-160\n",
       "Time:                        14:08:27   Log-Likelihood:                -1560.8\n",
       "No. Observations:                 280   AIC:                             3242.\n",
       "Df Residuals:                     220   BIC:                             3460.\n",
       "Df Model:                          59                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         41.7204      1.264     32.995      0.000      39.228      44.212\n",
       "x1          -282.6646      9.922    -28.489      0.000    -302.218    -263.111\n",
       "x2           618.3454      7.755     79.736      0.000     603.062     633.629\n",
       "x3            -2.0472      5.751     -0.356      0.722     -13.381       9.287\n",
       "x4           -16.8637      4.903     -3.440      0.001     -26.526      -7.201\n",
       "x5            -3.9881      5.190     -0.768      0.443     -14.216       6.240\n",
       "x6            -3.2633      2.427     -1.345      0.180      -8.046       1.519\n",
       "x7             3.2633      2.427      1.345      0.180      -1.519       8.046\n",
       "x8             8.7963      0.535     16.437      0.000       7.742       9.851\n",
       "x9            -8.7963      0.535    -16.437      0.000      -9.851      -7.742\n",
       "x10          -17.4252      2.143     -8.130      0.000     -21.649     -13.201\n",
       "x11           17.4252      2.143      8.130      0.000      13.201      21.649\n",
       "x12          -13.3873      1.381     -9.692      0.000     -16.110     -10.665\n",
       "x13          -11.1752      1.430     -7.817      0.000     -13.992      -8.358\n",
       "x14           21.2968      1.848     11.526      0.000      17.655      24.938\n",
       "x15           52.0410     12.418      4.191      0.000      27.568      76.514\n",
       "x16         -178.1063     23.913     -7.448      0.000    -225.234    -130.979\n",
       "x17            4.8286      9.372      0.515      0.607     -13.641      23.298\n",
       "x18            6.8128      8.843      0.770      0.442     -10.616      24.241\n",
       "x19           -5.8480      9.133     -0.640      0.523     -23.847      12.151\n",
       "x20           -0.2192      4.205     -0.052      0.958      -8.507       8.068\n",
       "x21            0.2192      4.205      0.052      0.958      -8.068       8.507\n",
       "x22           20.9759      4.507      4.654      0.000      12.093      29.858\n",
       "x23          -20.9759      4.507     -4.654      0.000     -29.858     -12.093\n",
       "x24            1.4720      4.478      0.329      0.743      -7.352      10.296\n",
       "x25           -1.4720      4.478     -0.329      0.743     -10.296       7.352\n",
       "x26           -5.7664      6.071     -0.950      0.343     -17.731       6.199\n",
       "x27           10.7206      6.739      1.591      0.113      -2.561      24.002\n",
       "x28           -4.3684      5.131     -0.851      0.396     -14.481       5.744\n",
       "x29          157.9286     13.613     11.601      0.000     131.100     184.757\n",
       "x30          -15.1313      8.982     -1.685      0.093     -32.833       2.571\n",
       "x31          -12.6743      7.840     -1.617      0.107     -28.125       2.776\n",
       "x32            9.6102      8.140      1.181      0.239      -6.432      25.653\n",
       "x33            0.0451      3.913      0.012      0.991      -7.666       7.756\n",
       "x34           -0.0451      3.913     -0.012      0.991      -7.756       7.666\n",
       "x35          -28.9701      4.245     -6.824      0.000     -37.336     -20.604\n",
       "x36           28.9701      4.245      6.824      0.000      20.604      37.336\n",
       "x37           -0.2474      4.016     -0.062      0.951      -8.163       7.668\n",
       "x38            0.2474      4.016      0.062      0.951      -7.668       8.163\n",
       "x39           11.6327      5.921      1.965      0.051      -0.037      23.302\n",
       "x40           -8.6371      5.480     -1.576      0.116     -19.438       2.163\n",
       "x41           -2.5112      4.608     -0.545      0.586     -11.592       6.569\n",
       "x42            6.7745      3.299      2.053      0.041       0.272      13.277\n",
       "x43           -1.6839      5.534     -0.304      0.761     -12.591       9.223\n",
       "x44            1.6486      4.965      0.332      0.740      -8.136      11.433\n",
       "x45           -4.2154      2.495     -1.689      0.093      -9.133       0.702\n",
       "x46            4.2154      2.495      1.689      0.093      -0.702       9.133\n",
       "x47           -2.3328      2.867     -0.814      0.417      -7.984       3.318\n",
       "x48            2.3328      2.867      0.814      0.417      -3.318       7.984\n",
       "x49            3.7512      2.869      1.308      0.192      -1.902       9.405\n",
       "x50           -3.7512      2.869     -1.308      0.192      -9.405       1.902\n",
       "x51            1.0733      3.844      0.279      0.780      -6.502       8.648\n",
       "x52           -3.7196      3.435     -1.083      0.280     -10.489       3.050\n",
       "x53            2.3162      2.940      0.788      0.432      -3.479       8.111\n",
       "x54            1.8425      5.799      0.318      0.751      -9.587      13.272\n",
       "x55           -8.7533      5.024     -1.742      0.083     -18.654       1.147\n",
       "x56           -2.8018      2.487     -1.126      0.261      -7.704       2.100\n",
       "x57            2.8018      2.487      1.126      0.261      -2.100       7.704\n",
       "x58           -4.7847      3.082     -1.553      0.122     -10.858       1.289\n",
       "x59            4.7847      3.082      1.553      0.122      -1.289      10.858\n",
       "x60           -1.7467      2.541     -0.687      0.493      -6.755       3.262\n",
       "x61            1.7467      2.541      0.687      0.493      -3.262       6.755\n",
       "x62           -2.0501      3.272     -0.627      0.532      -8.499       4.399\n",
       "x63            1.5661      3.539      0.443      0.659      -5.409       8.541\n",
       "x64            0.4043      2.895      0.140      0.889      -5.300       6.109\n",
       "x65           -2.0565      4.435     -0.464      0.643     -10.798       6.684\n",
       "x66           -1.4020      2.580     -0.543      0.587      -6.487       3.683\n",
       "x67            1.4020      2.580      0.543      0.587      -3.683       6.487\n",
       "x68           -4.3904      3.372     -1.302      0.194     -11.036       2.255\n",
       "x69            4.3904      3.372      1.302      0.194      -2.255      11.036\n",
       "x70            2.1502      2.498      0.861      0.390      -2.772       7.073\n",
       "x71           -2.1502      2.498     -0.861      0.390      -7.073       2.772\n",
       "x72            5.0481      3.453      1.462      0.145      -1.757      11.853\n",
       "x73           -4.3256      3.548     -1.219      0.224     -11.318       2.666\n",
       "x74           -0.5864      2.989     -0.196      0.845      -6.477       5.305\n",
       "x75           41.4918      1.256     33.043      0.000      39.017      43.967\n",
       "x76          -41.4918      1.256    -33.043      0.000     -43.967     -39.017\n",
       "x77           -2.0976      1.318     -1.592      0.113      -4.695       0.500\n",
       "x78            2.0976      1.318      1.592      0.113      -0.500       4.695\n",
       "x79            0.8524      1.224      0.696      0.487      -1.560       3.265\n",
       "x80           -0.8524      1.224     -0.696      0.487      -3.265       1.560\n",
       "x81           -0.6453      1.685     -0.383      0.702      -3.966       2.675\n",
       "x82            1.5803      1.672      0.945      0.346      -1.715       4.876\n",
       "x83           -0.8206      1.441     -0.569      0.570      -3.661       2.020\n",
       "x84           41.4918      1.256     33.043      0.000      39.017      43.967\n",
       "x85            2.0976      1.318      1.592      0.113      -0.500       4.695\n",
       "x86           -2.0976      1.318     -1.592      0.113      -4.695       0.500\n",
       "x87           -0.8524      1.224     -0.696      0.487      -3.265       1.560\n",
       "x88            0.8524      1.224      0.696      0.487      -1.560       3.265\n",
       "x89            0.6453      1.685      0.383      0.702      -2.675       3.966\n",
       "x90           -1.5803      1.672     -0.945      0.346      -4.876       1.715\n",
       "x91            0.8206      1.441      0.569      0.570      -2.020       3.661\n",
       "x92           18.2636      0.629     29.027      0.000      17.024      19.504\n",
       "x93          -18.2636      0.629    -29.027      0.000     -19.504     -17.024\n",
       "x94           -0.2968      1.237     -0.240      0.811      -2.735       2.141\n",
       "x95            0.2968      1.237      0.240      0.811      -2.141       2.735\n",
       "x96           -1.2728      1.771     -0.719      0.473      -4.763       2.217\n",
       "x97            2.2096      1.670      1.323      0.187      -1.082       5.501\n",
       "x98           -0.8276      1.646     -0.503      0.616      -4.071       2.416\n",
       "x99           18.2636      0.629     29.027      0.000      17.024      19.504\n",
       "x100           0.2968      1.237      0.240      0.811      -2.141       2.735\n",
       "x101          -0.2968      1.237     -0.240      0.811      -2.735       2.141\n",
       "x102           1.2728      1.771      0.719      0.473      -2.217       4.763\n",
       "x103          -2.2096      1.670     -1.323      0.187      -5.501       1.082\n",
       "x104           0.8276      1.646      0.503      0.616      -2.416       4.071\n",
       "x105          33.6726      1.121     30.050      0.000      31.464      35.881\n",
       "x106         -33.6726      1.121    -30.050      0.000     -35.881     -31.464\n",
       "x107          -1.0348      1.700     -0.609      0.543      -4.385       2.316\n",
       "x108          -1.9918      1.870     -1.065      0.288      -5.678       1.694\n",
       "x109           2.6296      1.497      1.756      0.080      -0.321       5.580\n",
       "x110          33.6726      1.121     30.050      0.000      31.464      35.881\n",
       "x111           1.0348      1.700      0.609      0.543      -2.316       4.385\n",
       "x112           1.9918      1.870      1.065      0.288      -1.694       5.678\n",
       "x113          -2.6296      1.497     -1.756      0.080      -5.580       0.321\n",
       "x114          26.0549      1.610     16.181      0.000      22.881      29.228\n",
       "x115           0.2429      1.063      0.228      0.820      -1.853       2.339\n",
       "x116         -22.7005      1.188    -19.113      0.000     -25.041     -20.360\n",
       "x117          29.1571      1.561     18.682      0.000      26.081      32.233\n",
       "x118         -25.6269      1.155    -22.187      0.000     -27.903     -23.350\n",
       "x119          41.9333      1.275     32.899      0.000      39.421      44.445\n",
       "==============================================================================\n",
       "Omnibus:                        0.116   Durbin-Watson:                   1.823\n",
       "Prob(Omnibus):                  0.944   Jarque-Bera (JB):                0.175\n",
       "Skew:                          -0.047   Prob(JB):                        0.916\n",
       "Kurtosis:                       2.922   Cond. No.                     2.94e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.07e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
