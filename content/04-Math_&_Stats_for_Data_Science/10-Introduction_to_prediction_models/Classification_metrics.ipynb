{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "## Introduction\n",
    "In this section of the notebook, we will learn about the various methods in which our models can be judged for their strength, these are called Metrics. <br>\n",
    "Each Metric paints a different story of the model. Having the knowledge of multiple metrics allows a Data Scientist to have a larger understanding of how to measure accuracy in unique scenarios. <br>\n",
    "\n",
    "We will be looking over the following:-<br>\n",
    "<ol>\n",
    "    <li>Accuracy Score</li>\n",
    "    <li>Recall Score</li>\n",
    "    <li>Precision Score</li>\n",
    "    <li>F1 Score</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "One of the most useful tools for Metric estimation is the <b>Confusion Matrix</b>.<br>\n",
    "In order to understand its importance we need to observe in action. Therefore, we will begin by running a classification on a dataset. \n",
    "We will be using the Breast Cancer dataset which contains 13 features to determine if the datapoint results in cancer (1) or not (0). The process below is as follows:-\n",
    "<ol>\n",
    "    <li>Block 1: We will be just import the necessary libraries to run a simple classification</li>\n",
    "    <li>Block 2:</li> \n",
    "    <ol>\n",
    "        <li>We are simply spliting the dataset to create a training set and a test set.</li>\n",
    "        <li>Then we instantiate a classifier and then run a classification</li>\n",
    "        <li>In the end we acqure the predicted values and we use the results to make a confusion matrix</li>\n",
    "    </ol>\n",
    "    \n",
    "</ol>\n",
    "\n",
    "```Python\n",
    "################### Block 1 - Importing Libraries #############################################\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "################### Block 2 - Running a Classification #############################################\n",
    "X = pd.DataFrame(load_breast_cancer()['data']).iloc[:, :2].values\n",
    "y = load_breast_cancer()['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "cls = RandomForestClassifier(n_estimators=90)\n",
    "cls = cls.fit(X_train,  y_train.reshape((-1,)))\n",
    "y_pred = cls.predict(X_test)\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# No Exercise Here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### No Solution Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "A confusion Matrix is a data structure that helps users with estimating a model's performance. <br>It requires 2 objects <br>(1) A list of true values of the tested data points and<br> (2) A list of the predicted values of your model. <br>\n",
    "\n",
    "<img src='../../../images/Images/confusion_matrix.png' width=\"350\" height=\"200\">\n",
    "\n",
    "The above diagram depicts a confusion matrix for a dataset that has 2 classes. <br>\n",
    "From the diagram it can be observed that the matrix splits the number of correctly predicted values from the incorrect ones. <br>\n",
    "Lets begin from the True Values:-<br>\n",
    "We have <br>\n",
    "<ol>\n",
    "    <li>Negative</li>\n",
    "        With in True values (<b>Values which are established to be correct</b>) we have the values whose class is represented by being <b>Negative</b> (0)\n",
    "    <li>Positive</li>\n",
    "    With in True values we have the values whose class is represented by being <b>Positive</b> (1)\n",
    "</ol>\n",
    "\n",
    "and similarly in the Predicted value section, we see we have the negatively and positively predicted values <b>from the model</b>\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li><u>True Negative</u></li>\n",
    "<small>\n",
    "    These Values are <i>correctly</i> predicted values which are Negative\n",
    "</small>\n",
    "\n",
    "<li><u>False Negative</u></li>\n",
    "<small>\n",
    "    These Values are <i>incorrectly</i> predicted values which are supposed to be Positives but were classified as Negatives\n",
    "</small>\n",
    "\n",
    "<li><u>True Positive</u></li>\n",
    "<small>\n",
    "    These Values are <i>correctly</i> predicted values which are Positive\n",
    "</small>\n",
    "\n",
    "<li><u>False Positive</u></li>\n",
    "<small>\n",
    "    These Values are <i>incorrectly</i> predicted values which are supposed to be Negativses but were classifies as Positives\n",
    "</small>\n",
    "</ul>\n",
    "\n",
    "\n",
    "We can access the Confusion Matrix method from sklearn.metrics \n",
    "\n",
    "```Python\n",
    "print('Confusion Matrix\\n', mat)\n",
    "```\n",
    "<img src=\"../../../images/Images/mat.PNG\" width=\"150\" height=\"150\">\n",
    "\n",
    "<h4>The number of True Negatives: 57</h4>\n",
    "\n",
    "```Python\n",
    "TN = mat[0][0]\n",
    "```\n",
    "\n",
    "<h4>The number of True Positives: 111</h4>\n",
    "\n",
    "```Python\n",
    "TP = mat[0][1]\n",
    "```\n",
    "\n",
    "<h4>The number of False Negatives: 10</h4>\n",
    "\n",
    "```Python\n",
    "FN = mat[1][0]\n",
    "```\n",
    "\n",
    "<h4>The number of False Positives: 10</h4>\n",
    "\n",
    "```Python\n",
    "TP = mat[1][1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Exercises Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### No Solution Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluations\n",
    "\n",
    "Since, now we have access to a confusion matrix, we can now calculate different metrics based on the data we created previously.<br>\n",
    "We will be using the example matrix mat, we will be using it. Metrics that can be calculated with a confusion matrix: \n",
    "<br>\n",
    "<ol>\n",
    "<li>Accuracy Score:-</li> Gives the ratio of correctly predicted values against the the whole set of values<br>\n",
    "Score: (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "```Python\n",
    "accuracy_score = (mat[0][0] + mat[1][1])/(mat[0][0] + mat[0][1] + mat[1][0] + mat[1][1])\n",
    "```\n",
    "<li>Recall Score:-</li> Gives the ratio of Positively correctly predicted values against all the actual values which are positive<br>\n",
    "Score:  TP/(TP+FN)\n",
    "\n",
    "```Python\n",
    "recall_score = mat[1][1]/(mat[1][0]+mat[1][1])\n",
    "```\n",
    "<li>Precision Score:-</li> Gives the ratio of Positively correctly predicted values against all the Predicted values<br>\n",
    "Score:  TP/(TP+FP)\n",
    "\n",
    "```Python\n",
    "precision_score = mat[1][1]/(mat[1][1]+mat[0][1])\n",
    "```\n",
    "<li>F1 Score:-</li> It is a good metric to be relied on when the parts of the confusion_mtrix too one-sided <br>\n",
    "Score: (2* precision_score * recall)/(precision_score + recall_score)\n",
    "\n",
    "\n",
    "```Python\n",
    "f1_score = (2* precision_score * recall_score)/(precision_score + recall_score)\n",
    "```\n",
    "</ol>\n",
    "\n",
    "The above discussed scores are all ratios to get the percentage values all we need to do is multiply them by 100\n",
    "\n",
    "### Which Metric to select ?\n",
    "\n",
    "Each Metric can be crucial in its own way. \n",
    "Lets begin my understanding their formulae.\n",
    "<ol>\n",
    "    <li>Accuray Score:</li>\n",
    "    \n",
    "In accuracy we are basically are getting a simple approximation when we devide the correctly predicted by the whole number of datasets. But it doesn't tell you anything about how your predictor worked in independent instances, i.e. we cannot know from this metric alone understand how False positives can affect the whole structure of the result. So we look into Recall and Precision.\n",
    "\n",
    "<li>Recall Score</li>\n",
    "\n",
    "If we are looking to judge a models performance Recall score gives a much better undertanding because we are taking the relations between correct predicted positive datapoints and dividing them by all realistic(true) positive datapoints. <br>\n",
    "This ratio gives an understanding of how big of a problem can False Negative pose to the model. This precisely gives imphasis on the False Negative. \n",
    "\n",
    "<li>Precision Score</li>\n",
    "\n",
    "In precision we look from the classifier's perpective and try to understand how the percentage of correctly prediction or not predicted can pose a problem to the model.\n",
    "\n",
    "<li>F1 score</li>\n",
    "Usually you can use this metric instead of Presicion and recall but its maily used when there is a massive imbalance in the confusion matrix, i.e. the number of datapoints in each cell is too high than the other. \n",
    "</ol>\n",
    "\n",
    "<hr>\n",
    "Exercise: For the following established y_true and y_pred values calculate the\n",
    "accuracy_score, recall_score, precision_score, f1_score. <br>\n",
    "Display your values within 2 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(1)\n",
    "y_true = np.random.randint(2, size=20)\n",
    "y_pred = np.random.randint(2, size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "```Python\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.random.seed(1)\n",
    "y_true = np.random.randint(2, size=20)\n",
    "y_pred = np.random.randint(2, size=20)\n",
    "mat = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "accuracy_score = (mat[0][0] + mat[1][1])/(mat[0][0] + mat[0][1] + mat[1][0] + mat[1][1])\n",
    "recall_score = mat[1][1]/(mat[0][0]+mat[1][1])\n",
    "precision_score = mat[1][1]/(mat[1][1]+mat[1][0])\n",
    "f1_score = (2* precision_score * recall_score)/( precision_score + recall_score )\n",
    "print('Accuracy Score:  {0:.2f}'.format(accuracy_score))\n",
    "print('Recall Score:    {0:.2f}'.format(recall_score))\n",
    "print('Precision Score: {0:.2f}'.format(precision_score))\n",
    "print('F1 Score:        {0:.2f}'.format(f1_score))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve\n",
    "Reciever Operator Characteristics is used to signify a classifier's strength or performance. The metric formed by the curve is called the roc_auc_score which is basically the area under curve (auc). \n",
    "\n",
    "<img src=\"../../../images/Images/ROC_Curve.PNG\" width=\"650\" height=\"650\">\n",
    "\n",
    "The plot for a straight line is considered random which is the worst case for an ROC curve since the area is 0. Because in this curve we have FPR(X) and TPR(Y) which says that both are 50/50 and so that gives a probability of 1/2 which basically for the model means its random since taking a guess is also 1/2 or 50% probability of getting the right classification. \n",
    "All the other curves above it represent a better a score and the larger the curve the better is the model. \n",
    "\n",
    "Another notification is that the current curves are considered positive and if we were to have these curves on the lower side it would not have considered to be a bad classifier because theoretically if we were to inverse every answer in the predicted outputs we would get a good model since now its positive. \n",
    "\n",
    "<b>Your curve cannot be the linear regression in the middle.</b> As you may now have understood the area under the curve is area above the linear regression and the estimated curve.\n",
    "\n",
    "\n",
    "### Why is ROC Curve significant ?\n",
    "ROC is immune to the imbalance in the data. Instaces where you find it hard to judge through the discussed metrics, ROC can help conclude a realiable judgement. The reason is that it deals with 2 rates which are defined as <br>\n",
    "\n",
    "<b>Falsely Positive Rate: </b>\n",
    "<small>The of prediction of Positively classed datapoints as False, so basically incorrectly predicted datapoints.</small>\n",
    "\n",
    "<b>True Positive Rate: </b>\n",
    "<small>The rate of correctly predicting Positively classed datapoints as True.</small>\n",
    "\n",
    "These estimates are essentially created through a combination of probabilities and true values. The functions provided to us in the sklearn module allow us to derive a list of rates for each datapoints and then with those we can graph them over the X-Y plane. <br>\n",
    "We shall attempt this process in the next few sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Exercises Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## No Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC_AUC_Score\n",
    "The Area Under Curve Score gives a calculated ratio of area under the curve versus the whole. The area under curve is a representation of the 2 estimators the true positive rates and the false positive rate. <br>\n",
    "Let's continue assessing how we can calculate the AUC score with sklearn.\n",
    "\n",
    "We begin by looking at previous code where we had a Random Forest Tree Classifier to classify cadidates that have cancer or not. <br>\n",
    "\n",
    "We can observe that we had instantiated a classifier and predicted the output values. <br>\n",
    "But for the following exercise we shall continue after we had predicted the values since we need the ready model to make our ROC curve. \n",
    "\n",
    "<ul>\n",
    "    <li>Import all the necessary libraries</li>\n",
    "    <li>We are importing roc_curve() calculating the </li>\n",
    "    <li>auc() for calculating the score under the curve</li>\n",
    "    <li>And matplotlib for visualization and displaying the curve</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>.classes_ is a variable that stores your classes of the dataset in this case it was 2, 0 and 1</li>\n",
    "    <li>The way roc_curve works is that it needs probabilities of the predictions, i.e. probability of either the model will predict a datapoint to be either 0 or 1. </li>\n",
    "    <li> we use .predict_proba() to estimate the probabilities of each class and the object. </li>\n",
    "\n",
    "<img src=\"../../../images/Images/probs.PNG\" width=\"200\" height=\"400\">\n",
    "    \n",
    "  <li>We cannot use a 2D array and the probabilities need only be for the positive class which is 0, indicator for non-cancer hence, we slice it.</li>\n",
    "  \n",
    "<img src=\"../../../images/Images/probs1.PNG\" width=\"400\" height=\"200\">\n",
    "<li>The above picture is a list of probabilities</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```Python\n",
    "cls.classes_\n",
    "probs = cls.predict_proba(X_test)[:, 1]\n",
    "```\n",
    "\n",
    "<ul>\n",
    "    <li>We further then calculate the false and negative positive rates through the roc_curve function using true values (y_test) and the probabilities(probs)</li>\n",
    "    <li>Then proceed by calculating the area by the auc() function with the help of the rates</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "```Python\n",
    "f_p_r, t_p_r, ths = roc_curve(y_test, probs)\n",
    "area = auc(f_p_r, t_p_r)\n",
    "```\n",
    "<ul>\n",
    "    <li>Finally we have the True positive rate which goes on Y-axis and False Positive rate which goes on X-axis</li>\n",
    "</ul>\n",
    "\n",
    "```Python\n",
    "plt.title('ROC (AUC) Score = {0:.2f}'.format(area)  )\n",
    "plt.plot(f_p_r, t_p_r)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"../../../images/Images/diagram.PNG\" width=\"400\" height=\"400\">\n",
    "<hr>\n",
    "\n",
    "Exercise: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "y_true = np.random.randint(2, size=20)\n",
    "probs = np.random.randint(2, size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "```Python\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)\n",
    "y_true = np.random.randint(2, size=20)\n",
    "probs = np.random.randint(2, size=20)\n",
    "\n",
    "f_p_r, t_p_r, ths = roc_curve(y_true, probs)\n",
    "area = auc(f_p_r, t_p_r)\n",
    "\n",
    "plt.title('ROC (AUC) Score = {0:.2f}'.format(area)  )\n",
    "plt.plot(f_p_r, t_p_r)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
